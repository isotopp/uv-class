[{"id":0,"href":"/books/","title":"Python projects with uv","section":"Welcome","content":"Keks\n"},{"id":1,"href":"/books/uv-class/01-installation/","title":"Installing and Configuring uv","section":"Using uv to manage python projects","content":"Summary Choosing an installation method: Use the native installer if you want the simplest, most direct setup. Use Homebrew, Scoop, or Winget if you prefer centralized tool management. Use Cargo only if you are a Rust developer or contributor. All methods install the same uv binary. The difference is how updates and removal are handled.\nShell integration is optional but recommended. Completion improves discoverability and correctness.\nThis chapter establishes uv as a system tool, not just a project tool. Students learn how uv is installed, updated, and integrated into their shell, and where persistent configuration lives. The goal is to make uv feel reliable, discoverable, and non-magical from the start.\nUnderstanding installation and configuration early avoids later confusion about PATH issues, mismatched versions, or “why does uv behave differently on this machine”.\nInstalling uv# uv is distributed as a single, self-contained executable. You do not need Python, pip, or an existing virtual environment to install it. This makes installation fast, predictable, and easy to undo: uv is one single, self-contained binary that goes into the system path.\nThere are several supported installation methods. Which one you choose depends on your platform and how you manage system tools.\nUsing the native installer (recommended)# The native installer is the recommended method on all platforms. It installs a prebuilt uv binary and places it on your PATH.\nLinux and macOS# On Linux and macOS, use the official installer script:\ncurl -LsSf https://astral.sh/uv/install.sh | shWhat this does:\nDownloads a precompiled uv and uvx binary for your platform Installs it without privileges into a user-local directory On MacOS and Linux, it goes into $HOME/.local/bin. Updates your shell configuration so uv is on your PATH On MacOS and Linux, using bash, you end up with # uv export PATH=\u0026#34;$HOME/.local/bin:$PATH\u0026#34;at the end of $HOME/.bash_profile. After installation, restart your shell (or in bash, run hash -r) or reload your profile, then verify:\nwhich uv uv --version # or uv self versionThis method does not require administrator privileges and does not modify system Python installations, nor does it require a preinstalled version of Python on your system at all.\nWindows# On Windows, the native installer is provided as a PowerShell script:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34;This installs uv into a user-local directory and updates the PATH for your user account.\nAfter installation, open a new terminal and verify:\nuv --versionUpdating using the native installer# Run the command\nuv self update This can only work if uv is installed locally (is writeable by you), and if the uv version is natively installed. If you try this on a version installed by Homebrew or another managed install, it will not work:\n$ uv self update error: uv was installed through an external package manager and cannot update itself. hint: You installed uv using Homebrew. To update uv, run `brew update \u0026amp;\u0026amp; brew upgrade uv`Installing on MacOS with Homebrew# If you manage developer tools with Homebrew, you can install uv that way instead.\nbrew install uvLater, upgrade with brew upgrade uv.\nUse this method if\nYou already manage most tools with Homebrew You prefer system-wide package management Installing on Windows with Scoop or Winget# On Windows, uv is also available through common package managers.\nscoop install uvScoop installs uv into your user profile and manages PATH automatically.\nwinget install astral-sh.uvWinget installs uv system-wide and integrates with Windows package management.\nChoose Scoop or Winget if you already rely on them for other tools.\nUpdates are done using the native method for your package manager.\nInstalling with Cargo (for Rust developers)# If you are a Rust developer or want to work on uv itself, you may prefer installing using Cargo.\ncargo install uv This method is not recommended for general users, but it is appropriate if you:\ncontribute to uv want to track development versions are already managing Rust tools with Cargo Verifying the installation# Regardless of how you installed uv, always verify:\n$ uv --version uv 0.9.16 (Homebrew 2025-12-06)or uv self version --output-format json:\n{ \u0026#34;package_name\u0026#34;: \u0026#34;uv\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.9.16\u0026#34;, \u0026#34;commit_info\u0026#34;: { \u0026#34;short_commit_hash\u0026#34;: \u0026#34;Homebrew\u0026#34;, \u0026#34;commit_hash\u0026#34;: \u0026#34;Homebrew\u0026#34;, \u0026#34;commit_date\u0026#34;: \u0026#34;2025-12-06\u0026#34;, \u0026#34;last_tag\u0026#34;: null, \u0026#34;commits_since_last_tag\u0026#34;: 0 } }Shell/Powershell Completion (optional step)# Once uv is installed, integrating it with your shell improves usability and discoverability. This is not required to use uv, but it significantly reduces friction, especially when learning the tool or exploring less frequently used commands.\nLinux and macOS shells# uv supports shell completion for: bash, zsh, fish. To generate completion scripts, use:\nuv generate-shell-completion bashor replace \u0026ldquo;bash\u0026rdquo; with the name of your shell. The command prints the completions to standard output. You typically redirect this output into the appropriate completion directory for your shell. After installing completions, restart your shell or reload its configuration.\nWindows Powershell# On Windows, uv can generate PowerShell completion scripts as well.\nuv generate-shell-completion powershellThis outputs a PowerShell script that enables tab completion for uv. Typically, you append this output to your PowerShell profile file so it loads automatically:\nuv generate-shell-completion powershell \u0026gt;\u0026gt; $PROFILEAfter restarting PowerShell, tab completion for uv commands becomes available.\nConfiguration# uv can be configured in three different ways: command-line flags, configuration files, and environment variables. All three exist to support different use cases, from one-off commands to persistent, shared configuration.\nThere is a hierarchy of configuration option sources:\nCommand-line flags override everything Configuration files provide persistent defaults Environment variables act as a fallback and integration mechanism Not every setting exists in all three forms, but many important ones do. When multiple forms exist, they are designed to map cleanly to each other.\nCommand-line flags# Command-line flags are the most explicit and visible way to configure uv.\nExamples:\nuv run --python 3.12 uv sync --frozen uv add --group dev ruffFlags\napply to a single invocation are easy to discover via uv help always take precedence Configuration files# Configuration files allow you to set persistent defaults without repeating flags on every command.\nuv supports configuration files at multiple levels:\nper-project configuration as a section in the pyproject.toml file of the project. per-user configuration via the uv.toml file. Inside the pyproject.toml, uv uses the prefix tool.uv, so the configuration of PyPI indexes becomes\n[[tool.uv.index]] url = \u0026#34;https://test.pypi.org/simple\u0026#34; default = trueIn the uv.toml, the prefix is not used:\n[[index]] url = \u0026#34;https://test.pypi.org/simple\u0026#34; default = trueuv will search for pyproject.toml or uv.toml files in the current directory and its parents. uv will also discover uv.toml configuration files in the user- and system-level configuration directories, e.g., user-level configuration in ~/.config/uv/uv.toml, and system-level configuration at /etc/uv/ uv.toml on macOS and Linux.\nIf multiple files are found, the options are merged in the expected way: project-level configuration take precedence over the user-level configuration, and user-level configuration takes precedence over the system-level configuration\nRun uv --no-config to ignore any existing config-files. Run uv --config-file=... to use a specific configuration file in uv.toml format, there will be no search and all other config files will be ignored.\nEnvironment variables# Environment variables exist primarily for CI systems, automation and container environments. They are also useful when configuration must be injected externally.\nThe uv run subcommand can read .env files, which are turned into environment variables inside the run, which can be useful for own projects.\necho \u0026#34;MY_VAR=\u0026#39;Hello, world!\u0026#39;\u0026#34; \u0026gt; .env uv run --env-file .env -- python -c \u0026#39;import os; print(os.getenv(\u0026#34;MY_VAR\u0026#34;))\u0026#39; Hello, world!Relationship between flags, config, and environment# There is no strict rule that every option has a command-line flag, a configuration file entry and an environment variable. However, for core behavior, the mapping is usually consistent:\na long-form flag, for example --python a configuration key with the same name, python an environment variable prefixed with UV_, UV_PYTHON. So, --python 3.12, [uv.tool]-section python = \u0026quot;3.12\u0026quot; and environment variable UV_PYTHON=3.12 correspond.\nCommon options# There is a number of options that are common to all subcommands. They are listed in the uv help output and in all help sections of all subcommands.\nAmong them are the --managed-python family of options, which will download and install a specific python version managed by uv, making an existing installation of Python optional, and forgoes using the system python.\nThere are --quiet and --verbose options, and there is a --color option controlling output colorization.\nThere is a --directory option, which does cd into a directory first thing the comman starts.\nWe already mentioned --config-file ... and --no-config.\nThere are TLS options and options to completely disable network access.\nExercises# 0. Installation (mandatory)# Install uv on your computer and prove that it works by displaying its version and the path to the executable.\n1. Updating uv (easy)# How do you update uv if you installed it using the native installer? How does this differ if you used a package manager like Homebrew or Scoop?\n2. Configuration Hierarchy (easy)# List the three primary ways uv can be configured. In what order of precedence are they applied if the same setting is defined in all three?\n3. Shell Integration (medium)# Generate the shell completion script for your preferred shell (e.g., bash, zsh, or powershell). Identify where this script needs to be placed or sourced on your system to make completions persistent across sessions.\n4. Custom Configuration (medium)# Create a uv.toml file in a temporary directory that sets a specific Python version (e.g., 3.12) as the default. Use uv --config-file \u0026lt;path\u0026gt; or run uv from that directory to prove that it respects this configuration.\n5. Transfer: Strategy Design (hard)# A development team wants to ensure all members use the same Python version and the same internal package index. However, their CI/CD pipeline needs to use a different, faster mirror for the index. Propose a configuration strategy using pyproject.toml, uv.toml, and environment variables that satisfies these requirements while minimizing manual configuration for developers.\n"},{"id":2,"href":"/books/uv-class/","title":"Using uv to manage python projects","section":"Python projects with uv","content":"Summary uv is a tool for managing Python projects. It includes dependency management, but it does not stop there.\nTo explain uv properly, we need to explore how Python projects are created, maintained, and distributed throughout their entire lifecycle.\nWhat we do# This guide explains how to structure and maintain modern Python projects using uv. Our audience is students or Python junior developers.\nWhat we assume you know# We assume basic familiarity with Python, sufficient to follow code examples and understand the projects we build.\nWe assume you work with an IDE and have access to a Python installation.\nWe will use git to manage our codebase and Markdown for documentation. We assume you are comfortable with basic git operations and understand fundamental Markdown concepts.\nWhat we teach# Our goal is to explain modern Python project structure and the workflows required to create and maintain Python projects over time, and how uv supports that, providing consistency and reproducibility across different versions of Python.\nWe are biased toward native Python. We do not cover modules written in C or other languages, nor how to manage projects that depend on them.\nWe are opinionated. We favor src-structured packages created with uv init --package over flat Python projects, and we explain why.\nWe use entry points and starter scripts to expose packages as command-line programs.\nWe discuss versioning, packaging, installation, and distribution of Python packages, and how uv supports these workflows.\n"},{"id":3,"href":"/books/uv-class/02-what-is-uv/","title":"What uv is and how it works","section":"Using uv to manage python projects","content":"Summary Everything in uv follows the same pattern: declare intent in pyproject.toml, let uv resolve and lock dependencies in uv.lock, execute inside isolated environments that are recreated on the fly.\nWhat uv actually does# This chapter explains what uv actually does, beyond the shorthand “it installs dependencies”.\nAt its core, uv is a project-centric orchestration tool. It coordinates Python versions, environments, dependencies, tools, builds, and publishing in a way that is fast, reproducible, and difficult to misuse.\nThe key ideas to keep in mind throughout this chapter are:\nenvironments are isolated environments are reproducible environments are disposable creating and updating environments is cheap Once these ideas are understood, most uv commands become self-explanatory.\nWhat an environment is# A Python environment is a concrete, runnable Python context made up of:\nA specific Python interpreter and stdlib, the standard library that comes with that interpreter A set of installed third-party packages Metadata describing how those packages were installed uv manages all of these pieces together, instead of leaving them to separate tools and conventions.\nPython version# Every environment is built around one specific Python version, such as 3.11, 3.12 or 3.14. The version determines language features, behavior of built-in modules, ABI compatibility, and which third-party wheels can be installed.\nuv can install and manage Python interpreters themselves, or use the operating system Python interpreter.\nNever use the system python. Always use a --managed-python with uv.\nUsing uv to manage python versions avoids the use of the system-managed system-python. A Python provided by the operating system may contain modules on top of stdlib, which may have been installed as OS packages to enable the system python to do system python things. This makes it specific to your OS version and update level.\nThis Python is also managed and updated by the system, which means it will change over time and outside your control. This is generally ruining reproducibility, for example, your system python and its environment will not match a CI environment – different bugs, different behavior.\nUsing uv with managed python versions gives you clean python environments with no preinstalled packages beyond stdlib, allows you to have multiple versions (for example, for automated testing), and makes sure the same python version is used everywhere (for example, on your machine and in CI).\nUsing uv with --managed-python is the preferred way to use uv and Python.\nWhen you request a Python version that is not present, uv can download and install it. The interpreter comes with its matching standard library. The Python version is fully isolated from system Python installations. This is similar in spirit to tools like pyenv, but integrated directly into the project workflow.\nuv does not install system libraries, compilers, operating system packages or non-Python runtimes.\nFor example, if a Python package requires a system library (such as OpenSSL or a C compiler), that must be provided by the operating system or container, or you need to use something like conda.\nVirtual environments# A virtual environment is a directory that contains:\na link to a Python interpreter (or a copy thereof) an isolated site-packages directory with the packages needed Python metadata pointing to those locations It exists to ensure that packages installed for one project do not affect another.\nuv creates and manages virtual environments automatically, implicitly and very quickly. You do not run python -m venv. You do not activate environments. You do not manually install packages.\nInstead:\nuv sync ensures the environment matches the lockfile uv run executes commands inside the environment The environment is treated as a derived artifact, not something you curate by hand. It is also completely disposable: You do not normally notice uv throwing away or rebuilding your virtual environment. It is just there, always with exactly the tools, libraries and versions you configured.\nIntent, Resolution, Lockfile# The environment contains Python packages. The collection of Python packages on top of stdlib are called the project dependencies. Each dependency has a version attached to it.\nIn the pyproject.toml, each dependency is declared with a range of acceptable versions for it. Usually (and by default) that is \u0026ldquo;any version newer than the version we used when we started using the dependency\u0026rdquo;. This looks like\ndependencies = [ \u0026#34;httpx\u0026gt;=0.28.1\u0026#34;, ]This is declarative, and expresses our intention: We say what packages we need, and what versions are acceptable. We do not say which exact version is to be used.\nuv resolves dependencies by looking at the totality of all declared dependencies, even those that are optional or separated out into other dependency groups.\nIt then tries to find versions that are compatible: For example, when one component depends on httpx\u0026gt;=0.28.1, and another component we declared transitively depends on httpx\u0026gt;=0.26.3, uv will choose a version of httpx\u0026gt;=0.28.1, because that is compatible with all packages that want httpx.\nThe outcome of the resolution is written to uv.lock and kept unchanged. There, the exact version, file source and sha checksum are recorded. The results from the uv.lock are used in a sync or run.\nThe uv.lock file is a derived file, but it must be checked in, so that the exact same versions of packages are being used in every checkout by you, your colleages and the CI.\nThis guarantees that all users of our project will use the exact same versions of all dependencies at all times, making runs identical and reproducible, across developer machines, CI/CD environments and installations.\nThis is also why all dependency groups are taken into account all of the time, even those that are not relevant for this exact install: We do not want the version of httpx that is being used to change depending on the dev dependencies being installed or not.\nA Python project can have many dependencies. uv allows us to group them into separate groups, the default group being what is needed to run the project, and additional groups typically being things such as dev (containing ruff and mypy) or test (containing pytest).\nAn uv.lock file will not change once it is created. Upgrading versions is a conscious and reversible decision, and done with the uv lock command.\nWhat problem uv solves# Traditional Python workflows accumulate state over time:\nenvironments are created manually and forgotten environments are shared between projects dependencies are installed incrementally and drift tools are installed globally and differ per machine Developers end up with a \u0026ldquo;works on my machine\u0026rdquo; situation, and fragile codebases: Because virtual environments are not managed down to the checksum-level there may be subtle differences between different machines, containers or other environments. The development environment and the deployment environment can diverge, the process becomes fragile, and instead of debugging code we debug environments.\nuv prevents that by avoiding this situation structurally: each project is treated as the unit of truth, not the machine.\nIt also makes the environment disposable, recreated on the fly and completely dependent on the pyproject.toml and the resolved uv.lock file. This creates freshly created, up-to-date and statically resolved environments that are upgraded to new versions in a controlled and reversible process.\nFrom setup.py and requirements.txt to pyproject.toml# Python projects historically lack a single, authoritative definition: metadata in setup.py embedded in executable Python code, dependencies in requirements.txt and dev tools in requirements-dev.txt, plus a number of scripts scattered across files in undefined locations.\nThis fragmentation makes it hard to understand what a project actually is, much less parse this in a reliable way.\nuv centers the project around pyproject.toml: project metadata lives in one place, dependencies are declared explicitly, scripts and entry points are defined once in a central place and all tooling reads the same configuration.\nuv init creates a project that already fits this model, instead of requiring manual cleanup later.\nuv does not replace version control systems like git (but initializes a repo and creates a skeleton .gitignore), documentation conventions (but creates an empty README.md as a starting point), application-specific project structure decisions (but creates the necessary scaffolding).\nUsing uv habitually, you can\u0026rsquo;t forget to activate the environment, won\u0026rsquo;t accidentally install dependencies into the global interpreter, won\u0026rsquo;t have environment drift, and don\u0026rsquo;t have to wait for environment recreation.\nFor uv, environments are derived artifacts and not precious state. They are created automatically, recreated if necessary, cheap to discard and rebuilt, and not \u0026ldquo;activated\u0026rdquo;.\nUsing uv, projects are always isolated from each other. Environments are never shared or re-used, state does not leak. Multiple versions of environments, such as \u0026ldquo;the same environment with different Python versions\u0026rdquo; can be created and used.\nUsing dependency groups, uv also records tooling used, and tool versions. Tools are (an optionally installable) part of the environment, they are dependencies.\nThis ensures identical tooling and tool versions across all developers (put your preferences into your own group!), identical tooling in CI, reproducible results, and predictable behavior.\nIt collapses several categories into one:\nrunning the application running tests running linters running formatters are all just commands executed inside a known environment.\nuv does not replace pytest, ruff or mypy, but it makes sure they are installed in defined versions, and run with the correct interpreter across machines.\nBeyond development and execution# Packaging is often misunderstood as something separate from development: build tools feel opaque, publishing feels risky, metadata and build logic are scattered. Many developers avoid packaging entirely because it seems complex, they recommend running a git checkout.\nuv integrates packaging into the same lifecycle: project metadata is already present, build dependencies are declared explicitly, builds run in isolated environments, artifacts are reproducible.\nCommands like uv build, uv publish and uv tool install . use the same configuration and environment model as everything else.\nuv does not replace build backends such as: setuptools, hatchling, scikit-build-core or meson-python. Instead, it acts as a frontend, invoking these backends in a controlled, isolated way.\nSpeed and caching# In uv, an environment is treated as disposable, and the environment is very frequently recreated. This needs to be fast.\nuv uses a global cache, and wheels are cached across projects, resolved dependencies are reused, builds avoid unnecessary work. This cache is, by default, a local directory.\n$ uv cache dir ~/.cache/uv $ uv cache size --human warning: `uv cache size` is experimental and may change without warning. Pass `--preview-features cache-size` to disable this warning. 352.9MiBThe cache prevents multiple downloads of wheels and packages, allowing recreation of a good-sized venv in a few hundred milliseconds \u0026ldquo;on the fly\u0026rdquo;.\nThis is the foundation of uv\u0026rsquo;s speed and isolation.\nExercises# 1. Core Concepts (Reproduction)# What are the four key ideas about environments that uv promotes (as listed in the beginning of this chapter)? Briefly explain what \u0026ldquo;environments are disposable\u0026rdquo; means in the context of uv.\n2. The Role of the Lockfile (Reproduction)# Why is it mandatory to check the uv.lock file into version control? What is the difference between \u0026ldquo;intent\u0026rdquo; (in pyproject.toml) and \u0026ldquo;resolution\u0026rdquo; (in uv.lock)?\n3. Investigating the Cache (Application)# Run uv cache dir to find your local cache directory. Then, run uv cache size --human (you might need to enable preview features as shown in the chapter). How much space is your uv cache currently using?\n4. Environment Isolation (Application)# Create two new projects in separate directories using uv init. Add different dependencies to each (e.g., uv add requests in one and uv add httpx in the other). Use uv run python -c \u0026quot;import requests\u0026quot; and uv run python -c \u0026quot;import httpx\u0026quot; in both directories. What happens, and how does this demonstrate isolation?\n5. The \u0026ldquo;Works on My Machine\u0026rdquo; Problem (Transfer)# A colleague says: \u0026ldquo;I don\u0026rsquo;t need uv.lock, I just use requirements.txt with versions like requests\u0026gt;=2.0.0. It works fine on my machine.\u0026rdquo; Based on what you learned about environment drift and reproducibility, explain to them why this approach might lead to failures in CI/CD or on a teammate\u0026rsquo;s machine.\n"},{"id":4,"href":"/books/uv-class/03-python-projects/","title":"A sample project and a walkthrough","section":"Using uv to manage python projects","content":"Summary A walkthrough of simple uv usage with a primitive project to show what things look like during project creation, inside the dev project, and during build, install and version changes.\nThis walkthrough uses a tiny command line app as the learning vehicle. The goal is not the weather-app. Instead we use it as a vehicle to show how to set up a uv based project and how to run it.\nCreate a project# mkdir berlin-weather cd berlin-weather uv init --no-managed-python --python 3.10 --packageFor example:\n$ mkdir berlin-weather $ cd berlin-weather $ uv init --no-managed-python --python 3.10 --package Initialized project `berlin-weather` $ tree . |-- README.md |-- pyproject.toml `-- src `-- berlin_weather `-- __init__.py 3 directories, 3 files $ cat src/berlin_weather/__init__.py def main() -\u0026gt; None: print(\u0026#34;Hello from berlin-weather!\u0026#34;) $ ls -ld .??* drwxr-xr-x 9 kris wheel 288 Jan 4 17:25 .git -rw-r--r-- 1 kris wheel 109 Jan 4 17:21 .gitignore -rw-r--r-- 1 kris wheel 5 Jan 4 17:21 .python-version $ cat .gitignore # Python-generated files __pycache__/ *.py[oc] build/ dist/ wheels/ *.egg-info # Virtual environments .venv $ cat .python-version 3.10 $ git status On branch main No commits yet Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) .gitignore .python-version README.md pyproject.toml src/ nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track)We are running the command uv init with the option --package in an empty directory. It creates a project named after the directory, autoconverting - in the package-name to _ to accommodate python naming rules.\nWe also specified that we do not want uv to download a version of Python for us (we could also use --managed-python instead, which is usually preferable). And we specified a relatively old, custom version of Python, 3.10.\nThree files were created:\nREADME.md (empty), pyproject.toml (basic content) and module: src/berlin_weather/__init__.py with a hello-world code-fragment. The uv init also initialized an empty git repo for us, and a basic .gitignore.\nThe file .python-version describes the minimum python-version to be used.\nTesting the execution# We can run this with any allowed python version. Since we said 3.10, we could run this with 3.10 or newer. Let\u0026rsquo;s try 3.13 instead:\n$ uv run --no-managed-python --python 3.13 python -c \u0026#34;from berlin_weather import main; main()\u0026#34; Using CPython 3.13.11 interpreter at: /opt/homebrew/opt/python@3.13/bin/python3.13 Creating virtual environment at: .venv Built berlin-weather @ file:///private/tmp/berlin-weather Installed 1 package in 2ms Hello from berlin-weather!We observe:\nuv casually created a .venv directory and installed all dependencies in it (one, the Python itself so far). We ran the code from the package. We use uv init --package# We could have simply used uv init to create a flat structure.\nWe prefer --package:\nIt makes the project installable and buildable from day one. It defaults to a src layout, which avoids accidental imports from the working directory. It gives us modules and the opportunity to define entry points that can be called from loader scripts under [project.scripts] and that will become command line binaries in the .venv path. So instead of \u0026ldquo;a folder of python files\u0026rdquo; we have a project with proper modules and dependencies.\nOther uv init modes exist: Flat layout (\u0026ldquo;folder of python files\u0026rdquo;) and script mode (metadata in the header comments of a single python file), but we don\u0026rsquo;t go there. Use uv help init for the details.\nWe get src/berlin_weather/__init__.py. The directory name is the module name, and module names (identifiers) in Python can\u0026rsquo;t contain minus-signs. Instead the minus is converted to an underbar (_). To make the directory an importable Python module, the file __init__.py must exist. It may be empty.\nIn our case, it contains a callable main() function as a placeholder for testing.\nA basic pyproject.toml# We also get a basic pyproject.toml:\n$ cat pyproject.toml [project] name = \u0026#34;berlin-weather\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; authors = [ { name = \u0026#34;Kristian Koehntopp\u0026#34;, email = \u0026#34;kris-git@koehntopp.de\u0026#34; } ] requires-python = \u0026#34;\u0026gt;=3.10\u0026#34; dependencies = [] [project.scripts] berlin-weather = \u0026#34;berlin_weather:main\u0026#34; [build-system] requires = [\u0026#34;uv_build\u0026gt;=0.9.21,\u0026lt;0.10.0\u0026#34;] build-backend = \u0026#34;uv_build\u0026#34;uv init filled in some basic metadata, a version number, a placeholder description and a pointer to the README.md file. It lists the author metadata, finding it by reading the git configuration for the current user.\nThe list of project dependencies is empty. The Python version requirement is from the uv init commandline, where we specified --python 3.10.\nuv init also defined the default build-system (the built-in uv build system) to build packages. Other build backends are possible, and we can change that. For pure Python projects that is not necessary, but if we want to build something that extends CPYthon in other languages such as Rust or C, we have to.\nThe dummy main() function in our package is listed as an entrypoint, which means that a build, and install or an uv run will find a script berlin-weather in the path tha calls main() in the berlin_weather` module.\nRunning the application through the entrypoint script# $ uv run berlin-weather Using CPython 3.10.19 Creating virtual environment at: .venv Built berlin-weather @ file:///private/tmp/berlin-weather Installed 1 package in 2ms Hello from berlin-weather! $ uv run --python 3.14 berlin-weather Using CPython 3.14.2 Removed virtual environment at: .venv Creating virtual environment at: .venv Installed 1 package in 2ms Hello from berlin-weather!We have been calling the application through that entrypoint script: uv run berlin-weather.\nTo make things more interesting we specify a Python version, and observe how uv casually recreates the .venv as needed. Our application is run twice, once with Python 3.10, the other time with 3.14.\nAdding some code# To turn this into a proper application with an external dependency, we write a small piece of code that loads the current weather for Berlin from BrightSky using the httpx module from PyPi. httpx is a successor library to request, and can make REST calls or other HTTP requests.\nThe code looks like this:\nfrom __future__ import annotations import sys from dataclasses import dataclass from datetime import date from typing import Any import httpx BRIGHTSKY_WEATHER_URL = \u0026#34;https://api.brightsky.dev/weather\u0026#34; @dataclass(frozen=True) class WeatherNow: time: str temperature_c: float | None wind_speed_ms: float | None def _pick_latest_weather(payload: dict[str, Any]) -\u0026gt; WeatherNow: weather = payload.get(\u0026#34;weather\u0026#34;) or [] if not weather: raise RuntimeError(\u0026#34;No weather data returned.\u0026#34;) latest = weather[-1] return WeatherNow( time=str(latest.get(\u0026#34;timestamp\u0026#34;) or \u0026#34;\u0026#34;), temperature_c=latest.get(\u0026#34;temperature\u0026#34;), wind_speed_ms=latest.get(\u0026#34;wind_speed\u0026#34;), ) def _fetch_weather(lat: float, lon: float, day: date) -\u0026gt; WeatherNow: timeout = httpx.Timeout(10.0, connect=5.0) headers = {\u0026#34;User-Agent\u0026#34;: \u0026#34;berlin-weather-training/1.0\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;} params = {\u0026#34;lat\u0026#34;: lat, \u0026#34;lon\u0026#34;: lon, \u0026#34;date\u0026#34;: day.isoformat()} with httpx.Client(timeout=timeout, headers=headers, follow_redirects=True) as client: resp = client.get(BRIGHTSKY_WEATHER_URL, params=params) resp.raise_for_status() payload: dict[str, Any] = resp.json() return _pick_latest_weather(payload) def main() -\u0026gt; int: berlin_lat = 52.52000 berlin_lon = 13.40500 today = date.today() try: now = _fetch_weather(berlin_lat, berlin_lon, today) except (httpx.HTTPError, RuntimeError) as e: print(f\u0026#34;Error: {e}\u0026#34;, file=sys.stderr) return 2 t = \u0026#34;?\u0026#34; if now.temperature_c is None else f\u0026#34;{now.temperature_c:.1f} C\u0026#34; w = \u0026#34;?\u0026#34; if now.wind_speed_ms is None else f\u0026#34;{now.wind_speed_ms:.1f} m/s\u0026#34; print(f\u0026#34;Berlin. {now.time}. {t}. Wind {w}.\u0026#34;) return 0 if __name__ == \u0026#34;__main__\u0026#34;: raise SystemExit(main())The module calls main() and returns any error, if there is one.\nThe main() function defines the geocoordinates for Berlin and the current date. It then tries to call _fetch_weather() with these data. If that succeeds, the location, time, temperature and wind speed are being printed.\n_fetch_weather() does a call to the BRIGHTSKY_WEATHER_URLwith several request parameters, and a defined timeout. If there is a result, we parse it with_pick_latest_weather()`.\n_pick_latest_weather() takes the last field from the result, which is the latest (current) weather. It selects the data we want into a WeatherNow() dataclass and returns this. This allows us to retrieve this information in an ordered and structured way.\nAdding a dependency with uv add# The important piece of this code it that it makes use of httpx to make the call, which is not a library from stdlib. It is an external, installable dependency.\nWe declare this dependency with uv add httpx. This modifies pyproject.toml and then re-generates uv.lock.\n$ cat pyproject.toml [project] name = \u0026#34;berlin-weather\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; authors = [ { name = \u0026#34;Kristian Koehntopp\u0026#34;, email = \u0026#34;kris-git@koehntopp.de\u0026#34; } ] requires-python = \u0026#34;\u0026gt;=3.10\u0026#34; dependencies = [ \u0026#34;httpx\u0026gt;=0.28.1\u0026#34;, ] [project.scripts] berlin-weather = \u0026#34;berlin_weather.cli:main\u0026#34; [build-system] requires = [\u0026#34;uv_build\u0026gt;=0.9.21,\u0026lt;0.10.0\u0026#34;] build-backend = \u0026#34;uv_build\u0026#34;The dependencies-array is no longer empty. It now contains a version spec of httpx\u0026gt;=0.28.1. This is the current version of httpx at the moment we define this dependency.\nWe also manually changed the entrypoint to berlin_weather.cli:main (the function main() in src/berlin_weather/cli.py).\nRunning the script# We can run the script with different Python versions:\n$ uv run --python 3.14 -- berlin-weather Using CPython 3.14.2 Removed virtual environment at: .venv Creating virtual environment at: .venv Installed 7 packages in 4ms Berlin. 2026-01-05T00:00:00+00:00. -1.3 C. Wind 13.0 m/s. $ uv run --python 3.10 -- berlin-weather Using CPython 3.10.19 Removed virtual environment at: .venv Creating virtual environment at: .venv Installed 9 packages in 5ms Berlin. 2026-01-05T00:00:00+00:00. -1.3 C. Wind 13.0 m/s.We observe how the .venv is recreated with different versions of Python. We also observe how the number of packages installed is different.\nThe uv.lock file# Whenever we change the dependencies of a script, when we run uv.lock and in a few other situations the uv.lock file is recreated. It contains the actual dependencies of our script – the ones we specified, and the transitive, implied dependencies, taking different Python versions into account.\nThe precise source files, checksums and other details are recorded.\nWhen an uv.lock file exists, exactly the versions in the uv.lock file are being installed, guaranteeing the same versions of everything everywhere.\n$ head uv.lock version = 1 revision = 3 requires-python = \u0026#34;\u0026gt;=3.10\u0026#34; [[package]] name = \u0026#34;anyio\u0026#34; version = \u0026#34;4.12.0\u0026#34; source = { registry = \u0026#34;https://pypi.org/simple\u0026#34; } dependencies = [ { name = \u0026#34;exceptiongroup\u0026#34;, marker = \u0026#34;python_full_version \u0026lt; \u0026#39;3.11\u0026#39;\u0026#34; }, { name = \u0026#34;idna\u0026#34; }, { name = \u0026#34;typing-extensions\u0026#34;, marker = \u0026#34;python_full_version \u0026lt; \u0026#39;3.13\u0026#39;\u0026#34; }, ] sdist = { url = \u0026#34;https://files.pythonhosted.org/packages/16/ce/8a777047513153587 e5434fd752e89334ac33e379aa3497db860eeb60377/anyio-4.12.0.tar.gz\u0026#34;, hash = \u0026#34;sha256 :73c693b567b0c55130c104d0b43a9baf3aa6a31fc6110116509f27bf75e21ec0\u0026#34;, size = 22826 6, upload-time = \u0026#34;2025-11-28T23:37:38.911Z\u0026#34; } wheels = [ { url = \u0026#34;https://files.pythonhosted.org/packages/7f/9c/36c5c37947ebfb8c7f22e 0eb6e4d188ee2d53aa3880f3f2744fb894f0cb1/anyio-4.12.0-py3-none-any.whl\u0026#34;, hash = \u0026#34; sha256:dad2376a628f98eeca4881fc56cd06affd18f659b17a747d3ff0307ced94b1bb\u0026#34;, size = 113362, upload-time = \u0026#34;2025-11-28T23:36:57.897Z\u0026#34; }, ] [[package]] name = \u0026#34;berlin-weather\u0026#34; ...The uv.lock file has a header that declares the uv.lock file version, and the Python version requirement.\nIt then has a number of [[package]] entries (in toml, a [] is a table, an object, and [[]] means to append to the table, here the table package).\nWe see the entry for the package anyio, which is required by httpx, and the beginnning of the entry for the berlin-weather module itself.\nWe record the exact version, the source registry where we found the package, the list of dependencies added by this module, the sdist url and checksum and other metadata, and the wheels metadata, again with download url and checksum. This nails down the version installed, and with the checksum ensures that it is really the exact same file.\nThe anyio dependency is interesting, because it has transitive dependencies, and two of them are conditional: If we are below 3.11, we also need exceptiongroup, and if we are below 3.13, we need typing-extensions as well. This explains the different number of packages, depending on Python version.\nCheck in the uv.lock file# In order to ensure all environments are using the exact same packages, we put the uv.lock file under source control.\n$ git add uv.lock kris$ git commit -m \u0026#39;dependency update\u0026#39; [main (root-commit) 60e4d5e] dependency update 1 file changed, 104 insertions(+) create mode 100644 uv.lockBuilding, Installing and Versioning# Entrypoint# We changed the entrypoint to\n[project.scripts] berlin-weather = \u0026#34;berlin_weather.cli:main\u0026#34;That is, the script berlin-weather is created and will now call src/berlin_weather/cli.py, and specifically the main() function in there.\nWe can check:\n$ ls -l .venv/bin/berlin-weather -rwxr-xr-x 1 kris wheel 328 Jan 4 22:38 .venv/bin/berlin-weather $ cat .venv/bin/berlin-weather #!/private/tmp/berlin-weather/.venv/bin/python # -*- coding: utf-8 -*- import sys from berlin_weather.cli import main if __name__ == \u0026#34;__main__\u0026#34;: if sys.argv[0].endswith(\u0026#34;-script.pyw\u0026#34;): sys.argv[0] = sys.argv[0][:-11] elif sys.argv[0].endswith(\u0026#34;.exe\u0026#34;): sys.argv[0] = sys.argv[0][:-4] sys.exit(main())The script is small Python script, running with the interpreter from the .venv created by uv. It imports the entrypoint function from the entrypoint module, and then calls it.\nTo be able to use entrypoint scripts we need to write our code as an importable module, which is what the --package in uv init --package ensures.\nBuild system and build# Our pyproject.toml also contains a build system:\n[build-system] requires = [\u0026#34;uv_build\u0026gt;=0.9.16,\u0026lt;0.10.0\u0026#34;] build-backend = \u0026#34;uv_build\u0026#34;This enables us to do\n$ uv build Building source distribution (uv build backend)... Building wheel from source distribution (uv build backend)... Successfully built dist/berlin_weather-0.1.0.tar.gz Successfully built dist/berlin_weather-0.1.0-py3-none-any.whlWe get a .whl, a Python wheel (a built and installable \u0026ldquo;binary\u0026rdquo;, specific to a Python Version, an ABI and a platform). Our wheel is pure python, hence the ABI and playform are \u0026ldquo;none\u0026rdquo; and \u0026ldquo;any\u0026rdquo;, respectively. But for a CPython extension it may as well be \u0026ldquo;cp310-cp37-manylinux\u0026rdquo; (CPython 3.10, a specific variant of the CPython 3.7 ABI and diverse Linux platforms).\nThe .tar.gz is a source distribution, enabling to build the wheel locally yourself.\nInstalling# We can install the berlin-weather project with uv install . (install from the local directory).\n$ uv tool install . Resolved 7 packages in 27ms Built berlin-weather @ file:///private/tmp/berlin-weather Prepared 1 package in 4ms Installed 7 packages in 4ms + anyio==4.12.0 + berlin-weather==0.1.0 (from file:///private/tmp/berlin-weather) + certifi==2026.1.4 + h11==0.16.0 + httpcore==1.0.9 + httpx==0.28.1 + idna==3.11 Installed 1 executable: berlin-weather $ which berlin-weather /Users/kris/.local/bin/berlin-weather $ uv tool uninstall berlin-weather Uninstalled 1 executable: berlin-weatherThis installed the package with all dependencies in ~/.local, including the entrypoint script, which ends up in ~/.local/bin, which should be in our $PATH.\nWe can uninstall all this with uv tool uninstall berlin-weather.\nVersioning# Our pyproject.toml mentions a version number:\n$ cat pyproject.toml [project] name = \u0026#34;berlin-weather\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; authors = [ { name = \u0026#34;Kristian Koehntopp\u0026#34;, email = \u0026#34;kris-git@koehntopp.de\u0026#34; } ] requires-python = \u0026#34;\u0026gt;=3.10\u0026#34; dependencies = [ \u0026#34;httpx\u0026gt;=0.28.1\u0026#34;, ] [project.scripts] berlin-weather = \u0026#34;berlin_weather.cli:main\u0026#34; [build-system] requires = [\u0026#34;uv_build\u0026gt;=0.9.21,\u0026lt;0.10.0\u0026#34;] build-backend = \u0026#34;uv_build\u0026#34;Here, the version is 0.1.0:\nversion = \u0026#34;0.1.0\u0026#34;A new version of the tool berlin-weather will only be installed when the version number changes.\nWe can bump the version number of our project easily with uv version:\n$ grep version pyproject.toml version = \u0026#34;0.1.0\u0026#34; $ uv version --bump minor Resolved 9 packages in 26ms Built berlin-weather @ file:///private/tmp/berlin-weather Prepared 1 package in 3ms Uninstalled 1 package in 0.94ms Installed 1 package in 2ms - berlin-weather==0.1.0 (from file:///private/tmp/berlin-weather) + berlin-weather==0.2.0 (from file:///private/tmp/berlin-weather) berlin-weather 0.1.0 =\u0026gt; 0.2.0 $ grep version pyproject.toml version = \u0026#34;0.2.0\u0026#34;And when we install this, we can see how the isolated environment maintained by the uv tool install function gets changed:\n$ uv tool install . Resolved 7 packages in 2ms Built berlin-weather @ file:///private/tmp/berlin-weather Prepared 1 package in 4ms Uninstalled 1 package in 0.92ms Installed 1 package in 2ms - berlin-weather==0.1.0 (from file:///private/tmp/berlin-weather) + berlin-weather==0.2.0 (from file:///private/tmp/berlin-weather) Installed 1 executable: berlin-weatherand\n$ ls -l ~/.local/share/uv/tools/berlin-weather/lib/python3.14/site-packages/ total 24 -rw-r--r-- 1 kris staff 18 Jan 4 23:03 _virtualenv.pth -rw-r--r-- 1 kris staff 4342 Jan 4 23:03 _virtualenv.py drwxr-xr-x 15 kris staff 480 Jan 4 23:03 anyio drwxr-xr-x 10 kris staff 320 Jan 4 23:03 anyio-4.12.0.dist-info drwxr-xr-x 4 kris staff 128 Jan 4 23:06 berlin_weather drwxr-xr-x 11 kris staff 352 Jan 4 23:06 berlin_weather-0.2.0.dist-info drwxr-xr-x 7 kris staff 224 Jan 4 23:03 certifi drwxr-xr-x 9 kris staff 288 Jan 4 23:03 certifi-2026.1.4.dist-info drwxr-xr-x 14 kris staff 448 Jan 4 23:03 h11 drwxr-xr-x 9 kris staff 288 Jan 4 23:03 h11-0.16.0.dist-info drwxr-xr-x 14 kris staff 448 Jan 4 23:03 httpcore drwxr-xr-x 8 kris staff 256 Jan 4 23:03 httpcore-1.0.9.dist-info drwxr-xr-x 21 kris staff 672 Jan 4 23:03 httpx drwxr-xr-x 9 kris staff 288 Jan 4 23:03 httpx-0.28.1.dist-info drwxr-xr-x 11 kris staff 352 Jan 4 23:03 idna drwxr-xr-x 8 kris staff 256 Jan 4 23:03 idna-3.11.dist-infoThat is, for each installed tool, uv maintains an isolated venv for production in ~/.local/share/uv/tools/berlin-weather. In this particular venv, the version of berlin_weather has now been changed to 0.2.0.\nAdding dev and test dependencies# Our new code not only has a runtime dependency on httpx, but we also want to use ruff for source code formatting (replacing black), and for linting (replacing pylint). We still use mypy for type checking, because ty is not yet production quality. These are dev-Dependencies, with dev being a dependency group.\nWe also want to use pytest for testing, and put that in either the dev dependency group, or into a special test dependency group. The latter allows the installation of only pytest in CI, while the full set of dev is not needed for CI.\nWe can add arbitrary dependency groups.\n$ uv add --group test pytest Resolved 16 packages in 63ms Built berlin-weather @ file:///private/tmp/berlin-weather Prepared 2 packages in 16ms Uninstalled 1 package in 0.94ms Installed 7 packages in 13ms ~ berlin-weather==0.2.0 (from file:///private/tmp/berlin-weather) + iniconfig==2.3.0 + packaging==25.0 + pluggy==1.6.0 + pygments==2.19.2 + pytest==9.0.2 + tomli==2.3.0 $ uv add --dev mypy ruff Resolved 21 packages in 54ms Built berlin-weather @ file:///private/tmp/berlin-weather Prepared 3 packages in 336ms Uninstalled 1 package in 0.67ms Installed 6 packages in 11ms ~ berlin-weather==0.2.0 (from file:///private/tmp/berlin-weather) + librt==0.7.7 + mypy==1.19.1 + mypy-extensions==1.1.0 + pathspec==0.12.1 + ruff==0.14.10We add pytest to the group test. We add mypy and ruff to the group dev.\nA group is always specified as --group \u0026lt;name\u0026gt;. The group dev is special, we can address it as --group dev, but also as --dev.\nOur pyproject.toml now grew a new table [dependency-groups] with subtables for each group:\ncat pyproject.toml [project] name = \u0026#34;berlin-weather\u0026#34; version = \u0026#34;0.2.0\u0026#34; description = \u0026#34;Add your description here\u0026#34; readme = \u0026#34;README.md\u0026#34; authors = [ { name = \u0026#34;Kristian Koehntopp\u0026#34;, email = \u0026#34;kris-git@koehntopp.de\u0026#34; } ] requires-python = \u0026#34;\u0026gt;=3.10\u0026#34; dependencies = [ \u0026#34;httpx\u0026gt;=0.28.1\u0026#34;, ] [project.scripts] berlin-weather = \u0026#34;berlin_weather.cli:main\u0026#34; [build-system] requires = [\u0026#34;uv_build\u0026gt;=0.9.21,\u0026lt;0.10.0\u0026#34;] build-backend = \u0026#34;uv_build\u0026#34; [dependency-groups] dev = [ \u0026#34;mypy\u0026gt;=1.19.1\u0026#34;, \u0026#34;ruff\u0026gt;=0.14.10\u0026#34;, ] test = [ \u0026#34;pytest\u0026gt;=9.0.2\u0026#34;, ]Resolution (the creation of the uv.lock file) will always take all dependencies in all groups into account, even if we unselect certain dependency groups. This is to ensure package versions stay stable and do not change just because we deselect testing or something. If we didn\u0026rsquo;t do that, we\u0026rsquo;d have code that may work, but only when test is installed, because that selects a non-broken version of some package somewhere.\nWe can use uv sync to install no, some or all dependency groups:\n$ uv sync --no-group test --no-dev Resolved 21 packages in 7ms Audited 9 packages in 0.53msWe deselect the group test, and the group dev. We could have written -no-group dev, but for dev there is also special syntax.\n$ uv sync --no-group test --dev Resolved 21 packages in 6ms Installed 6 packages in 17ms + librt==0.7.7 + mypy==1.19.1 + mypy-extensions==1.1.0 + pathspec==0.12.1 + ruff==0.14.10 + tomli==2.3.0We select the dev dependencies, but not the test dependencies.\n$ uv sync --all-groups Resolved 21 packages in 6ms Installed 5 packages in 8ms + iniconfig==2.3.0 + packaging==25.0 + pluggy==1.6.0 + pygments==2.19.2 + pytest==9.0.2We install all dependencies from all groups.\nTo run tools on our code, we need to run them in the environment, so uv run pytest, uv run mypy src and uv run ruff format src, uv run ruff checks --fix.\n$ uv run ruff format src 1 file reformatted, 1 file left unchanged $ uv run ruff check --fix All checks passed!What we get# In this walkthrough, we:\nInitialized a new Python project using uv init --package, establishing a robust src layout and an installable structure from the start. Developed a functional command-line application that fetches weather data from an external API (BrightSky) using httpx. Managed dependencies by adding httpx and observed how uv automatically handles transitive dependencies and environment markers in uv.lock. Demonstrated the flexibility of uv by running our application across multiple Python versions (3.10 and 3.14) with automatic virtual environment management. Built the project into a distributable wheel and source distribution using uv build. Installed and uninstalled the application as a standalone system tool using uv tool install. Automated version management with uv version --bump. Organized development and testing workflows by adding dev and test dependency groups for tools like ruff, mypy, and pytest. Ultimately, we get a professional, reproducible, and easily distributable Python project, all managed through a single, unified tool: uv.\nExercises# 1. Project Scaffolding (Reproduction)# Why does this walkthrough recommend using uv init --package instead of a simple uv init? List at least three benefits mentioned in the text.\n2. Entrypoints and Scripts (Reproduction)# In pyproject.toml, what is the purpose of the [project.scripts] section? Explain what berlin-weather = \u0026quot;berlin_weather.cli:main\u0026quot; tells uv to do during installation.\n3. Probing the Environment (Application)# Follow the walkthrough to create the berlin-weather project and add httpx. Then, run the app with two different Python versions (e.g., 3.10 and 3.13) using uv run --python \u0026lt;version\u0026gt;. Observe the output of uv carefully. Why does uv say \u0026ldquo;Removed virtual environment\u0026rdquo; and \u0026ldquo;Creating virtual environment\u0026rdquo; when you switch versions?\n4. Versioning and Tool Installation (Application)# Install your berlin-weather project as a tool using uv tool install .. Verify where the executable is placed using which berlin-weather. Now, use uv version --bump patch to change the version, and run uv tool install . again. Observe the output: how does uv handle the transition from the old version to the new one in the tool environment? Go into the .local directory, find the berlin-weather application in the path – what does it look like? The application runs from an isolated environment, a virtual environment in .local/share/uv/tools/berlin-weather. Explore this environment.\n5. Transitive Dependencies (Transfer)# Open the uv.lock file generated after adding httpx. Find the anyio package entry. Note the dependencies list for anyio. Explain why a project running on Python 3.10 might have more packages installed in its .venv than the same project running on Python 3.14, based on the marker fields in uv.lock.\n"},{"id":5,"href":"/books/uv-class/04-basics/","title":"General Options and TOML","section":"Using uv to manage python projects","content":"Summary uv\u0026rsquo;s Capabilities: uv handles Python versions, scripts, projects, tools, and provides a pip-compatible interface and cache management.\nGetting Help: Use uv --help, uv \u0026lt;command\u0026gt; --help, or uv help \u0026lt;command\u0026gt; to access documentation.\nCommon Options: Global flags like --no-cache, --managed-python, --offline, and --config-file work across most subcommands.\nTOML Basics: uv uses TOML for configuration. It supports basic types (strings, integers, booleans), tables for organization, and arrays of tables for lists of objects.\nTopics# What uv can do: A high-level overview of capabilities. Getting help: Navigating the built-in documentation and pagers. Verbose output: Using flags to see more detail. Common options: Global flags for cache, Python discovery, and configuration. TOML basics: Understanding the configuration format used by uv. What uv can do# uv is a versatile tool designed to handle almost every aspect of Python development. Below is an overview of its core capabilities.\nManage Python uv python ... Run Scripts uv run --script, uv add --script, uv remove --script. Manage Projects uv init, uv add, uv remove, uv sync, uv lock, uv run, uv tree, uv build and uv publish Run tools: uvx, uv tool run, uv tool install, uv tool uninstall, uv tool list, uv tool update-shell` Cosplay as PIP: uv pip ..., and uv pip compile, uv pip sync. Manage a cache, uv cache ..., and other maintenance options uv ... dir, uv self update. Manage Python# uv can download, install, and manage multiple Python versions automatically. Using uv python ... commands, you can list available versions, install specific ones, and ensure your projects always run on the correct interpreter without relying on system-installed Python.\nRun Scripts# For standalone Python scripts, uv provides a seamless way to manage dependencies without a full project structure. Commands like uv run --script allow you to execute scripts with inline dependency metadata, while uv add --script and uv remove --script manage those dependencies directly within the script file.\nManage Projects# uv simplifies project orchestration from initialization to publishing. It handles scaffolding (uv init), dependency management (uv add, uv remove), environment synchronization (uv sync), and locking (uv lock). It also provides tools for inspecting dependency graphs (uv tree) and building or publishing packages (uv build, uv publish).\nRun Tools# uv makes it easy to run and install Python-based CLI tools in isolated environments. You can run tools instantly with uvx or uv tool run, and manage persistent installations with uv tool install, uv tool uninstall, and uv tool list. It also helps keep your environment integrated with uv tool update-shell.\nCosplay as PIP# To maintain compatibility with existing workflows, uv can act as a drop-in replacement for pip. The uv pip interface provides lightning-fast alternatives to common commands like uv pip install, uv pip compile for requirements files, and uv pip sync for environment synchronization.\nMaintenance and Cache# uv includes built-in tools for self-maintenance and performance optimization. You can manage the global package cache with uv cache ..., locate important directories using uv ... dir, and keep the tool itself up to date with uv self update.\nGetting help# uv provides comprehensive built-in documentation through its help system. You can access high-level help for the entire tool or detailed information for specific subcommands.\nTo see the main help text and a list of all available commands:\nuv --helpFor help on a specific subcommand, you can use either the --help flag after the command or the help subcommand:\nuv init --help # or uv help init # longer than --helpBecause uv commands often have many options, the help output can be quite long. By default, uv detects if you are in a terminal and may automatically use a pager (like less) for the output.\nIf you want to read the help output without a pager (for example, to copy it or if you are scripting), you can pipe the output to cat:\nuv help init | catConversely, if you want to explicitly use a pager, or if you are in an environment where uv doesn\u0026rsquo;t detect the terminal correctly, you can always pipe to less:\nuv run --help | lessVerbose output# uv supports a --verbose/-v option to increase the verbosity of output. This can be useful for debugging or understanding the steps uv is taking.\nFor example, to see detailed information about a command:\nuv sync -vThe option can be repeated multiple times to get even more verbose output, for example uv sync --vv. Usually, the verbose output also explains why uv is doing the things it does.\nCommon options# Many options in uv are \u0026ldquo;global\u0026rdquo; or \u0026ldquo;common\u0026rdquo;, meaning they work with almost every subcommand. These options are always listed at the end of the help output for any command. Because this list is long, using | less is particularly helpful to see all of them.\nCache options# uv uses a global cache to speed up operations. You can control how uv interacts with this cache:\n-n, --no-cache: Avoids reading from or writing to the cache. uv will use a temporary directory for the duration of the command. This is useful for troubleshooting or ensuring a clean run. --cache-dir \u0026lt;CACHE_DIR\u0026gt;: Allows you to specify a custom path for the uv cache, overriding the default location. Python options# These options control how uv discovers and manages Python interpreters:\n--managed-python / --no-managed-python: Explicitly enable or disable the use of Python versions managed by uv. --no-python-downloads: Prevents uv from automatically downloading new Python versions (but the cached versions are still being used). Global options# These are general-purpose flags that affect uv\u0026rsquo;s behavior across the board:\n-q, --quiet: Minimizes output. -v, --verbose: Increases output detail (can be used multiple times, e.g., -vv). --offline: Disables all network access, forcing uv to rely only on cached data. --directory \u0026lt;DIRECTORY\u0026gt;: Changes the working directory before running the command. --project \u0026lt;PROJECT\u0026gt;: Forces uv to look for a project in a specific directory. --config-file \u0026lt;CONFIG_FILE\u0026gt;: Specifies a custom uv.toml file to use. --no-config: Prevents uv from discovering any configuration files (pyproject.toml or uv.toml). --color \u0026lt;COLOR_CHOICE\u0026gt;: Controls whether to use colored output (auto, always, or never). TOML basics# uv uses TOML (Tom\u0026rsquo;s Obvious, Minimal Language) for its configuration files (pyproject.toml and uv.toml). TOML is designed to be easy to read and maps directly to a hash table (or JSON object).\nBasic types# TOML supports several basic data types:\nStrings: Always wrapped in double quotes. \u0026quot;Hello, world!\u0026quot; Integers: Plain numbers. 42, -10 Floats: Numbers with decimal points. 3.14 Booleans: true or false (lowercase). Arrays: Values in square brackets. [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;] Equivalent JSON:\n{ \u0026#34;name\u0026#34;: \u0026#34;uv-project\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;active\u0026#34;: true, \u0026#34;tags\u0026#34;: [\u0026#34;fast\u0026#34;, \u0026#34;reliable\u0026#34;] }Equivalent TOML:\nname = \u0026#34;uv-project\u0026#34; version = 1 active = true tags = [\u0026#34;fast\u0026#34;, \u0026#34;reliable\u0026#34;]Tables# Tables (often called dictionaries or maps in other languages) are defined by a header in square brackets. Everything following that header until the next header belongs to that table.\nEquivalent JSON:\n{ \u0026#34;project\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;berlin-weather\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34; } }Equivalent TOML:\n[project] name = \u0026#34;berlin-weather\u0026#34; version = \u0026#34;0.1.0\u0026#34;Inline tables# For small sets of data, you can use inline tables, which look like JSON objects but use = instead of :.\nauthors = [{ name = \u0026#34;Alice\u0026#34;, email = \u0026#34;alice@example.com\u0026#34; }]Arrays of tables# When you need a list of objects, TOML uses double square brackets [[table]]. Each time you use the double bracket header, it appends a new entry to the array.\nEquivalent JSON:\n{ \u0026#34;index\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;https://pypi.org/simple\u0026#34;, \u0026#34;default\u0026#34;: true }, { \u0026#34;url\u0026#34;: \u0026#34;https://test.pypi.org/simple\u0026#34; } ] }Equivalent TOML:\n[[index]] url = \u0026#34;https://pypi.org/simple\u0026#34; default = true [[index]] url = \u0026#34;https://test.pypi.org/simple\u0026#34;This is how uv handles multiple package indexes or complex dependency groups. Every [[index]] block adds another index to the list.\nExercises# 1. What uv can do# Reproduction: Name three different categories of tasks that uv can manage besides project dependencies. Reproduction: What command would you use to see a tree of your project\u0026rsquo;s dependencies? Application: You want to run a one-off tool that isn\u0026rsquo;t in your project. Which uv command is specifically designed for this, and what happens to the environment after the tool finishes? Application: How would you use uv to check if a new version of the uv executable itself is available and install it? 2. Getting help# Reproduction: What is the difference between uv --help and uv help \u0026lt;command\u0026gt;? Reproduction: How does uv decide whether to use a pager (like less) for its help output? Application: Write a command that displays the help for uv sync but forces the output to be displayed without a pager. Application: You are looking for a specific option in the help text of uv run. How can you paginate the output so you can search or scroll through it easily on a Unix-like system? 3. Verbose output# Reproduction: Which flag is used to increase the verbosity of uv\u0026rsquo;s output? Reproduction: How do you get even more detailed information if the single verbose flag isn\u0026rsquo;t enough? Application: Run a command like uv python list -v. What kind of additional information does uv provide that isn\u0026rsquo;t in the standard output? Application: Explain a situation where using -vv might be helpful when uv is failing to resolve a dependency. 4. Common options# Reproduction: Where in the help output are \u0026ldquo;Common options\u0026rdquo; typically listed? Reproduction: What does the --offline flag do, and what does it force uv to rely on? Application: You want to ensure uv uses a specific directory for its cache for a single command. Which flag would you use, and how would the command look? Application: Write a command that runs uv sync for a project located in ../other-project/ without first changing your current working directory in the shell. 5. TOML basics# Reproduction: What are the basic data types supported by TOML mentioned in this chapter? Reproduction: In TOML, what is the difference between a single bracket [table] and double brackets [[table]]? Application: Convert the following JSON object into its TOML equivalent: { \u0026#34;tool\u0026#34;: { \u0026#34;uv\u0026#34;: { \u0026#34;managed\u0026#34;: true, \u0026#34;python\u0026#34;: \u0026#34;3.12\u0026#34; } } } Application: You need to add multiple package indexes to your configuration. Show the TOML structure required to define an array of two index objects, each having a url and a default boolean. "},{"id":6,"href":"/books/uv-class/05-dependencies/","title":"Dependencies, environments, and reproducibility","section":"Using uv to manage python projects","content":"Summary Dependency Management: Use uv add and uv remove to manage dependencies. uv updates pyproject.toml and uv.lock simultaneously.\nLockfiles: The uv.lock file ensures reproducible environments across all machines by recording exact versions and hashes.\nSyncing: uv sync ensures the virtual environment matches the lockfile.\nDocker Optimization: Use flags like --no-install-project, --no-install-local, and --no-dev to optimize Docker layers and reduce image size.\nTopics# Adding and removing dependencies uv add uv remove Runtime vs development dependencies uv add --dev, uv remove --dev uv add --group \u0026lt;name\u0026gt;, uv remove --group \u0026lt;name\u0026gt; Dependency resolution When? What is considered? Versions, Version ranges, what is chosen? Lockfiles What uv.lock contains Why it exists Why lockfiles belong in git uv sync and uv run What they read Why lockfiles are authoritative Upgrading with uv lock Sharing projects and deterministic installs Dependencies and Docker Installs --no-install-project --no-install-local --no-dev --only-group \u0026lt;name\u0026gt; Layer caching strategies Installation Installer options: --link-mode, --reinstall, --compile-bytecode Cache options: --no-cache, --refresh Python discovery and --managed-python Python Projects and Dependencies# After getting the basics out of the way, let\u0026rsquo;s dive deeper into how uv handles dependencies and why it does things the way it does, using our berlin-weather project as a reference.\nAdding and removing dependencies# Managing dependencies in uv is primarily done through the uv add and uv remove commands.\nuv add# When we ran uv add httpx in the berlin-weather project, uv performed several actions:\nIt looked up httpx on PyPI. It found the latest compatible version. It updated the dependencies array in pyproject.toml. It resolved the entire dependency tree (including httpx\u0026rsquo;s own dependencies like anyio and httpcore). It updated the uv.lock file with the exact versions and checksums. It synchronized the virtual environment in .venv. Why this way? By updating both pyproject.toml (your intent) and uv.lock (the resolution) immediately, uv ensures that your development environment is always in sync with your declarations.\nVersion constraints# You may also specify version constraints in uv add:\nuv add \u0026#34;httpx\u0026gt;=0.23,\u0026lt;0.30\u0026#34;uv puts exactly this version specifier with constraints into pyproject.toml and uses this to resolve. This particular version constraint will allow uv to use httpx versions from 0.23 up to but not including 0.30.\nuv remove# Similarly to uv add, uv remove httpx would:\nRemove the entry from pyproject.toml. Re-resolve the project (removing any transitive dependencies that are no longer needed). Update uv.lock. Clean up the .venv. Runtime vs development dependencies# Not all dependencies are needed to run the application. Some are only needed for development or testing.\nA dependency is defined as \u0026ldquo;code that is required for the application or build process to function, but that you did not write.\u0026rdquo; Dependencies are imported through your codebase through download, from PyPI or other package repositories, Python also uses the term \u0026ldquo;index\u0026rdquo; for this.\nAll dependencies are declared in the pyproject.toml, and are assigned a dependency group. The uv dependency resolution process decides what exact version of a dependency is being downloaded from an index, and this decision is recorded in the uv.lock file.\nTypes of dependencies are:\nRuntime Dependencies# These are required for the application to function. In berlin-weather, httpx is a runtime dependency. They are stored in the [project.dependencies] section of pyproject.toml.\nDevelopment Dependencies (--dev)# Tools like ruff (for formatting) or mypy (for type checking) are not needed by the end-user of your weather app. By running uv add --dev ruff mypy, you tell uv to put these in a special dev group.\nThese are stored under [dependency-groups] in pyproject.toml. They are included in your local development environment by default. Dependency Groups (--group \u0026lt;name\u0026gt;)# You can create arbitrary groups for specific needs. --dev is just shorthand for the longer --group dev option, because about every projecy will need development dependencies. But we can create arbitrarily named groups and add dependencies to them.\nAnother commonly used group is test. We used uv add --group test pytest for our testing framework.\nThis allows a CI system to install only what it needs (e.g., uv sync --group test might be enough to run tests, avoiding the overhead of mypy or ruff if those are not run as part of the testing cycle).\nDependency resolution# Resolution is the process of finding a set of package versions that satisfy all constraints.\nWhen does it happen? Every time you add, remove, or run uv lock. uv is designed to be so fast that it can re-resolve the entire project frequently. What is considered? uv considers all dependency groups during resolution. Versions and Ranges By default, uv add uses the newest version of a library that is compatible with all requirements from all packages and the pyproject.toml. Different resolver strategies exist (--resolution \u0026lt;strategy\u0026gt; switch), discussed elsewhere. Why this way? Resolving all groups together prevents \u0026ldquo;dependency hell\u0026rdquo; where adding a test tool later breaks your runtime environment. If a conflict exists, uv finds it immediately rather than at deployment time. Also, the same versions of everything are installed at all times, so the behavior (and the bugs) of your project does not change when certain dependency groups are activated or deactivated. Result of Dependency Resolution# Resolution \u0026ldquo;Find a compatible set of versions\u0026rdquo; That means:\nTake the declared dependencies Take their dependencies and their dependencies dependencies and so on -\u0026gt; transitive dependencies Look at all version constraints Look at the Python Version constraints Find the newest set of dependencies that, taken together, fulfills all constraints.\nberlin-weather depends on httpx\nhttpx depends on httpcore and many others.\nWe also added dev and test dependencies.\n$ uv pip tree berlin-weather v0.1.0 └── httpx v0.28.1 ├── anyio v4.12.1 │ └── idna v3.11 ├── certifi v2026.1.4 ├── httpcore v1.0.9 │ ├── certifi v2026.1.4 │ └── h11 v0.16.0 └── idna v3.11 mypy v1.19.1 ├── librt v0.7.7 ├── mypy-extensions v1.1.0 ├── pathspec v1.0.2 └── typing-extensions v4.15.0 pytest v9.0.2 ├── iniconfig v2.3.0 ├── packaging v25.0 ├── pluggy v1.6.0 └── pygments v2.19.2 ruff v0.14.10The version numbers shown reflect the resolved environment, not the constraint set.\n\u0026ldquo;Resolution\u0026rdquo; is a graph problem and becomes hard very quickly in larger projects. It is not easy to solve for humans, which is why we write tools for it.\nLockfiles# The uv.lock file is what powers reproducibility in uv.\nWhat it contains Every single package in your environment (direct and transitive), its exact version, the source URL, and a cryptographic hash (SHA-256) of the file. Why it exists To eliminate \u0026ldquo;works on my machine.\u0026rdquo; It records exactly what was used during a successful resolution, no matter where it is being installed. Why it belongs in Git By checking in uv.lock, you ensure that every developer, every CI runner, and every production deployment uses the exact same bytes for every dependency. uv sync and uv run# These commands are consumers of the lockfile.\nAuthority The uv.lock file is authoritative. When you run uv sync, uv doesn\u0026rsquo;t look at PyPI to see if there\u0026rsquo;s a newer version; it looks at the lockfile and makes your .venv match it exactly. You can tell uv sync to activate or deactivate dependency groups, though. Automatic Sync uv run will automatically check if your pyproject.toml or uv.lock has changed and perform a hidden uv sync if needed. Why this way? This makes the virtual environment a \u0026ldquo;derived artifact.\u0026rdquo; You don\u0026rsquo;t \u0026ldquo;manage\u0026rdquo; the .venv; you manage the pyproject.toml and uv.lock, and uv makes the .venv happen. Upgrading with uv lock# To update your dependencies to newer versions allowed by your pyproject.toml constraints, use:\nuv lock --upgrade: Upgrades all dependencies to the latest compatible versions. uv lock --upgrade-package httpx: Upgrades only httpx (and its dependencies if necessary). This updates the uv.lock file, which you then test and commit.\nSharing projects and deterministic installs# When a teammate clones the berlin-weather repo, they just run uv sync.\nuv reads uv.lock. uv downloads the exact wheels (verified by hashes). uv creates a .venv identical to yours. This is a deterministic install. The result is guaranteed to be the same regardless of when or where the command is run.\nDependencies and Docker Installs# In a development environment, we usually want everything: the project, the runtime dependencies, and the development tools. But in a Docker container (especially for production), we want the opposite: as little as possible.\nuv sync provides several options to control exactly what gets installed, which is crucial for building small, efficient, and well-cached Docker images.\nLayer Caching and --no-install-project# Docker builds are faster when they can reuse previous layers. The most expensive part of a Python build is usually installing dependencies.\nIf we copy our entire project and then run uv sync, any small change to our source code will invalidate the Docker cache, forcing a full re-install of all dependencies.\nTo avoid this, we use the \u0026ldquo;dependency-first\u0026rdquo; pattern:\nCopy metadata only: Copy pyproject.toml and uv.lock first. Install dependencies: Run uv sync --frozen --no-cache --no-install-project. --frozen: Ensures that uv does not update the uv.lock file. It will fail if the lockfile is out of sync with pyproject.toml. This is critical for reproducible builds. --no-cache: Prevents uv from using the local cache. In Docker, avoiding the cache helps keep the image size down if we don\u0026rsquo;t manage a persistent cache volume. Instead, your environments cache on the network is being used, that is, images are pulled from artifactory or whatever else is configured. But no $HOME/.cache is being used. --no-install-project: Installs all dependencies into .venv but skips installing the project itself (the code in src/). Copy source code: Copy the rest of your application code. Final Sync: Run uv sync --frozen --no-cache. This will be nearly instant because the dependencies are already cached in a Docker layer. # Install dependencies first to leverage Docker layer caching COPY pyproject.toml uv.lock ./ RUN uv sync --frozen --no-cache --no-install-project # Now copy the source code and install the project COPY . . RUN uv sync --frozen --no-cacheDependencies change less often than the source code itself, so Docker layer caching can kick in and our build speed will be higher than if we did the install in the normal order (source code first, dependencies second).\nReducing Image Size with --no-dev and --only-group# Production images should not contain pytest, ruff, or mypy. These tools take up space and increase the attack surface of your container.\n--no-dev: Excludes the dev dependency group. --no-group \u0026lt;name\u0026gt;: Excludes a specific group (e.g., --no-group test). --only-group \u0026lt;name\u0026gt;: Installs only the specified group. Note that this excludes the base project.dependencies and the project itself. --no-install-local: Skips the project and any local path dependencies. This is ideal for caching only heavy third-party packages from PyPI in an early Docker layer. For a production web service, you might use:\nuv sync --frozen --no-dev --no-group testIf you want an image that contains only the tools needed for testing, you might use:\nuv sync --frozen --only-group testand use this as a base layer for the \u0026ldquo;dependencies first install\u0026rdquo; outlines above.\nSummary of Exclusions# Option Effect --no-install-project Skips installing the current project, but keeps its dependencies. --no-install-local Skips the project and all local/path dependencies. Best for caching. --no-dev Omits the dev group. --no-group \u0026lt;name\u0026gt; Omits a specific named group. --only-group \u0026lt;name\u0026gt; Installs ONLY the named group (omits project and base deps). --no-default-groups Skips the default dev group. By combining these flags, you can ensure that your CI environments get exactly the test tools they need, while your production images remain lean and focused.\nInstallation# While uv sync is the primary command to ensure your environment matches your lockfile, several options control how that installation happens. These options are crucial for optimizing performance, managing disk space, and ensuring reliability in different environments.\nInstaller Options# These flags control the mechanics of how packages are placed into your .venv.\n--link-mode# uv uses a global cache to avoid downloading the same packages repeatedly. The --link-mode option determines how packages are moved from this cache into your project\u0026rsquo;s site-packages:\nclone (default on macOS): Uses \u0026ldquo;copy-on-write\u0026rdquo; clones. It\u0026rsquo;s fast and doesn\u0026rsquo;t take extra disk space, but allows the OS to separate files if they are modified. It requires that the cache directory is on the same filesystem as the .venv. That means it will not work with the user home on the internal disk, and the work directory on an external USB or network drive. hardlink (default on Linux/Windows): Creates hard links. Extremely fast and uses zero additional space, but files are shared across all environments. But the .venv and the cache must be on the same filesystem for this to work. copy: Physically copies the files. Slower and uses more disk space, but ensures the environment is completely independent of the cache. symlink: Creates symbolic links. Fast, but dangerous: if you clear the uv cache, your virtual environments will break because the target files are gone. Use of this mode is discouraged. --reinstall and --reinstall-package# --reinstall: Forces uv to reinstall all packages even if they are already present. This is a \u0026ldquo;nuclear option\u0026rdquo; for fixing a corrupted environment. --reinstall-package \u0026lt;name\u0026gt;: Reinstalls only the specified package. --compile-bytecode# By default, Python compiles .py files to .pyc bytecode lazily (the first time they are imported). Using --compile-bytecode during uv sync forces this compilation for all packages immediately.\nPros: Faster application startup time. Cons: Longer installation time. Usage: Highly recommended for Docker images and CLI tools where startup latency matters. Cache Options# The uv cache is the heart of its speed. See where it is located with uv cache dir. By default, on MacOS and Linux, $HOME/.cache/uv.\n--no-cache: Tells uv to ignore the global cache entirely for the current operation. In Docker: Use this to keep your image small if you aren\u0026rsquo;t using a persistent cache mount. Debugging: Use this to rule out cache corruption issues. --refresh: Invalidates all cached data and re-downloads everything. --refresh-package \u0026lt;name\u0026gt;: Refreshes the cache for a specific package. In a normal development workflow, you rarely need these. But in a CI/CD pipeline, --refresh ensures you are getting the absolute latest files if a package was re-published with the same version (which shouldn\u0026rsquo;t happen, but sometimes does in private registries).\nExercises# 1. Python Projects and Dependencies# Reproduction: What is the difference between [project.dependencies] and [dependency-groups] in pyproject.toml? Reproduction: How do you add a dependency to a specific group named test? Application: You want to add requests but only versions in the 2.x range starting from 2.31.0. Write the uv add command for this. 2. Dependency Resolution# Reproduction: When does uv perform dependency resolution? Reproduction: Does uv resolve only the active dependency groups, or all of them? Why? Application: If package A requires httpx\u0026gt;=0.20 and package B requires httpx\u0026lt;0.28, what version range will uv consider for httpx? 3. Lockfiles# Reproduction: Name three things recorded in uv.lock for every package. Reproduction: Why should uv.lock be committed to version control? Application: You suspect a colleague is using a different version of a transitive dependency than you are. How can uv.lock help you prove this or resolve the discrepancy? 4. uv sync and uv run# Reproduction: What is the relationship between uv.lock and uv sync? Reproduction: Why is it said that the .venv is a \u0026ldquo;derived artifact\u0026rdquo;? Application: You have just pulled changes from Git that include an updated uv.lock. What command should you run to ensure your local environment matches? 5. Dependencies and Docker Installs# Reproduction: What is the purpose of the --frozen flag in a Docker RUN command? Reproduction: What does --no-install-project do, and how does it help with Docker layer caching? Application: Write a uv sync command for a CI job that only needs the test dependency group and must not include the dev group or the main project code. Transfer: Design a multi-stage Dockerfile strategy for a uv-managed project. Your strategy should: Maximize caching of third-party dependencies. Result in a production image that contains only the runtime dependencies (no dev or test tools). Include pre-compiled Python bytecode for faster startup. Ensure the final image is as small as possible. Explain each step of your strategy and the flags used. 6. Installation# Reproduction: Explain the difference between clone and copy link modes. Which one is default on macOS? Reproduction: What is the benefit of using --compile-bytecode during installation? Application: You are working on a machine with very limited disk space and multiple uv projects. Which --link-mode would you choose to minimize disk usage, and what is the requirement for it to work? [//]: # (# TODO: project.dependencies, project.optional-dependencies (\u0026ndash;extras), dependency-groups, tool.uv.sources ) [//]: # (# TODO: Migration, \u0026ldquo;uv add -r requirements.txt\u0026rdquo;) [//]: # (# TODO: Platform specific dependencies: uv add \u0026quot;jax; sys_platform == 'linux'\u0026quot;, uv add \u0026quot;numpy; python_version \u0026gt;= '3.11'\u0026quot;; https://peps.python.org/pep-0508/#environment-markers)\n"},{"id":7,"href":"/books/uv-class/06-running/","title":"Running code and tools consistently","section":"Using uv to manage python projects","content":"Summary The uv run command is the primary way to execute code. It ensures a consistent environment is available, creating or updating it as needed.\nEnvironment Discovery: uv finds where it should run by looking for a pyproject.toml, or by creating an ephemeral environment for a single script.\nEditable Installs: By default, uv installs your project in \u0026ldquo;editable\u0026rdquo; mode so changes to source code are reflected immediately.\nTool Execution: uvx (or uv tool run) allows running tools in isolated, temporary environments without affecting your project.\nMultiple Environments: uv makes it trivial to test against multiple Python versions, either by switching the main environment or by maintaining side-by-side environments using UV_PROJECT_ENVIRONMENT.\nRunning Code with uv run# uv run is the Swiss Army knife of uv. It is the primary way to execute Python code, scripts, or project entrypoints while ensuring they run in the correct, isolated environment.\nThe most important thing to remember about uv run is that it is not just a wrapper. It manages the environment before execution. If your environment is out of sync with your pyproject.toml or uv.lock, uv run will automatically perform a sync in the background before starting your program.\nThis will automatically, quickly and invisibly ensure that your code runs in a fresh environment.\nSo\nwhile uv run (and uv sync) create a virtual environment named .venv (by default) and you can still .venv/bin/python your_script or .venv/bin/entrypoint_script, you should not do that. Because if you do, you are losing the \u0026ldquo;check the environment and rebuild it\u0026rdquo; step that uv run executes. Always run things with uv run, not directly from the virtual environment.\nWays to use uv run# uv run \u0026lt;something\u0026gt;# If you run a command that isn\u0026rsquo;t a Python script, uv looks for it in the virtual environment\u0026rsquo;s bin (or Scripts on Windows) directory, and only then in the rest of the system path.\n$ uv run bash bash-5.3$ echo $PATH ~/.local/bin:~/Source/uv-class/examples/whoami/.venv/bin:...In our berlin-weather project, we defined an entrypoint in pyproject.toml:\n[project.scripts] berlin-weather = \u0026#34;berlin_weather.cli:main\u0026#34;This creates an (editable) entrypoint script .venv/bin/berlin-weather.\nWe can run this simply with:\nuv run berlin-weatheruv ensures the .venv is up to date and then executes the berlin-weather script.\nIf you look closely at the PATH output above, you will see that ~/.local/bin is before .venv/bin. That means production versions of tools that are named the same as project entrypoints will be found first. So if you installed uv tool install . inside the berlin-weather project, you now have a ~/.local/bin/berlin-weather that will be used even when you work with uv run berlin-weather inside the project.\nuv run \u0026lt;script\u0026gt;.py# If the argument ends in .py, uv treats it as a Python script. It is effectively a shorthand for uv run python \u0026lt;script\u0026gt;.py.\nuv run whoami.py uv checks if the script contains a PEP-723 inline dependency declaration for scripts. If it is found, uv will also handle the scripts dependencies.\nuv run -m \u0026lt;module\u0026gt;# Just like python -m, this runs an installed module as a script.\nuv run -m json.tool blah.jsonuv run -# You can pipe Python code directly into uv run -. This executes the code using the project\u0026rsquo;s Python interpreter and environment.\necho \u0026#34;import httpx; print(httpx.__version__)\u0026#34; | uv run -Where the environment comes from# One of the \u0026ldquo;magic\u0026rdquo; aspects of uv run is how it decides which environment to use. It follows a specific discovery hierarchy:\nScript Metadata If you run uv run script.py and that script contains PEP 723 inline metadata, uv creates a temporary, ephemeral environment just for that script. Remote Scripts You can even run scripts directly from a URL: uv run https://raw.githubusercontent.com/astral-sh/uv/main/scripts/uv-elapsed.pyuv will download the script, look for metadata, create an environment, and run it.\nProject Discovery uv looks for a pyproject.toml in the current directory or any parent directory. If found, it uses the environment associated with that project (the .venv directory). If a virtual environment is already \u0026ldquo;active\u0026rdquo; in your shell (e.g., via source .venv/bin/activate), uv run will still prefer the environment it discovers via pyproject.toml. It is generally recommended not to activate environments when using uv.\nArgument Separation# When using uv run, you often need to pass parameters to the command you are running. uv parameters must come after the run and before the command that uv is suppsed to run.\nTo make things clearer, you can expressly end the uv parameters list usingthe double-dash -- separator:\n# uv run \u0026lt;uv params\u0026gt; -- \u0026lt;command\u0026gt; \u0026lt;command params\u0026gt; uv run --python 3.12 -- berlin-weather --verboseIn this example, --python 3.12 is for uv, while --verbose is passed to the berlin-weather application. Usually this is not needed,\nuv run --python 3.12 berlin-weather --verbosewill work just the same.\nEditable Installs# By default, when you work within a project, uv installs your project into the virtual environment in editable mode.\nWhat is \u0026ldquo;Editable\u0026rdquo;?# In traditional Python, if you install a package, its files are copied into site-packages. If you change your source code, you have to reinstall the package to see the changes.\nAn editable install (often called pip install -e .) creates a link (or a special .pth file) in site-packages that points back to your source directory.\nIn src layout (--package): uv handles the complexity of making the src/berlin_weather folder importable. It does that by adding the src directory to sys.path (in fact, it adds the directory named in the .pth file to the search path, and this happens to be the src directory). In flat layout: The current directory is made available. This is dangerous, because it can lead to unexpected behavior if you have files with the same name as modules in your project. That\u0026rsquo;s one of multiple reasons why we discourage the use \u0026ldquo;flat\u0026rdquo; layout in Python and prefer the src layout created with --package. An editable install means you can edit your code in src/berlin_weather/ cli.py and immediately see the effect when you run uv run berlin-weather, without any re-installation step.\nuv run --no-editable# Sometimes you want to test your project as if it were a regular, non-editable installation (e.g., to ensure you haven\u0026rsquo;t forgotten to include a file in your package).\nuv run --no-editable berlin-weatherThis forces uv to build a temporary wheel of your project and install it properly into the environment, rather than linking to your source code.\nTool Execution and uvx# Sometimes you want to run a tool (like ruff or pytest) that isn\u0026rsquo;t necessarily a dependency of your project, or you want to run it without setting up a project at all.\nWhat is uvx?# uvx is a convenient shorthand for uv tool run. It is designed for one-off tool execution.\nuvx ruff format .When you run this:\nuv creates an isolated, cached environment for ruff. It installs ruff into that environment. It executes the command. The environment is kept in the cache for future use, but it doesn\u0026rsquo;t clutter your project\u0026rsquo;s .venv. Common Tool Examples# Inside a project, you typically use uv run for tools declared in your dev group:\nTesting: uv run pytest Linting \u0026amp; Formatting: uv run ruff format src (Replaces black) uv run ruff check --fix src (Replaces pylint and isort) Type Checking: uv run mypy src Automatic Parameter Supply# In many projects, you find yourself running the same long commands. While uv doesn\u0026rsquo;t have a built-in \u0026ldquo;task runner\u0026rdquo; like npm run, it encourages the use of entrypoints or simple shell aliases.\nMultiple Environments# Because uv makes environment creation so cheap, you can easily test your code against multiple versions of Python.\nExample: Multi-version testing# If you want to ensure berlin-weather works on both Python 3.12 and 3.13:\nuv run --managed-python --python 3.12 --group test pytest uv run --managed-python --python 3.13 --group test pytestuv will create matching .venv states or cached environments for these calls, allowing you to switch back and forth instantly. This is a powerful way to verify compatibility without the complexity of managing multiple manual installations.\nSide-by-side environments# Sometimes, you don\u0026rsquo;t want uv to overwrite your main .venv every time you switch Python versions. You might want to keep your development environment in .venv (e.g., using Python 3.13) while having separate, persistent environments for testing against other versions.\nYou can achieve this by using the UV_PROJECT_ENVIRONMENT environment variable to point uv to a different directory:\n# Create and sync the 3.12 environment UV_PROJECT_ENVIRONMENT=.venv-3.12 uv sync --managed-python --python 3.12 --all-groups # Create and sync the 3.13 environment UV_PROJECT_ENVIRONMENT=.venv-3.13 uv sync --managed-python --python 3.13 --all-groups # Run tests using the respective environments UV_PROJECT_ENVIRONMENT=.venv-3.12 uv run --managed-python --python 3.12 --all-groups pytest UV_PROJECT_ENVIRONMENT=.venv-3.13 uv run --managed-python --python 3.13 --all-groups pytestBy setting UV_PROJECT_ENVIRONMENT, you tell uv to use that specific directory instead of the default .venv. This allows you to keep multiple virtual environments side-by-side in the same project directory, and switching between them is as simple as changing the environment variable. The default uv run --all-groups pytest (without the variable) will still use your main .venv directory.\nSimplifying multi-environment commands# Currently, uv does not have a built-in way to define these side-by-side environments in pyproject.toml or uv.toml. If you find yourself frequently typing these long commands, the recommended approach is to use shell aliases, a Makefile, or a task runner like just.\nOption 1: Shell Aliases# Add these to your .bashrc or .zshrc:\nalias uv-test-312=\u0026#39;UV_PROJECT_ENVIRONMENT=.venv-3.12 uv run --managed-python --python 3.12 --all-groups\u0026#39; alias uv-test-313=\u0026#39;UV_PROJECT_ENVIRONMENT=.venv-3.13 uv run --managed-python --python 3.13 --all-groups\u0026#39;Now you can simply run:\nuv-test-312 pytestOption 2: A simple Makefile# Create a Makefile in your project root:\n.PHONY: test-312 test-313 test-all test-312: UV_PROJECT_ENVIRONMENT=.venv-3.12 uv run --managed-python --python 3.12 --all-groups pytest test-313: UV_PROJECT_ENVIRONMENT=.venv-3.13 uv run --managed-python --python 3.13 --all-groups pytest test-all: test-312 test-313Then run make test-all to test against both versions.\nScripts# Python has a way to specify dependencies inline in scripts, initially defined in PEP-723 and now maintained in Inline Script Metadata.\nuv can work with such inline script metadata. Do make it do so, provide the option --script.\nExample script# #!/usr/bin/env python3 # Script at /examples/whoami/whoami.py in this repository, https://github.com/isotopp/uv-class # /// script # requires-python = \u0026#34;\u0026gt;=3.11\u0026#34; # dependencies = [ # \u0026#34;httpx\u0026gt;=0.26\u0026#34;, # ] # /// from datetime import datetime, timezone from email.utils import parsedate_to_datetime import httpx def main() -\u0026gt; None: with httpx.Client(timeout=10.0) as client: r = client.get(\u0026#34;https://httpbin.org/get\u0026#34;) r.raise_for_status() print(f\u0026#34;Your apparent IP: {r.json().get(\u0026#39;origin\u0026#39;, \u0026#39;unknown\u0026#39;)}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main()This short program calls https://httpbin.org/get to get the current IP.\nRunning a script# Running this script without uv or without the /// script header results in an error due to httpx missing.\n$ uv run whoami-no-dep.py Traceback (most recent call last): File \u0026#34;~/Source/uv-class/examples/whoami/whoami-no-dep.py\u0026#34;, line 6, in \u0026lt;module\u0026gt; import httpx ModuleNotFoundError: No module named \u0026#39;httpx\u0026#39;With the metadata block present, uv autodetects the metadata block (or we provide --script) and creates an ephemeral virtual environment with httpx installed.\n$ uv run --verbose whoami.py DEBUG uv 0.9.21 (0dc9556ad 2025-12-30) DEBUG Acquired shared lock for `~/.cache/uv` DEBUG Reading inline script metadata from `whoami.py` DEBUG Acquired exclusive lock for `~/Source/uv-class/examples/whoami/whoami.py` DEBUG No Python version file found in ancestors of working directory: ~/Source/uv-class/examples/whoami DEBUG Using Python request Python \u0026gt;=3.11 from `requires-python` metadata DEBUG Checking for Python environment at: `~/.cache/uv/environments-v2/whoami-0492c73d0afa970a` DEBUG The script environment\u0026#39;s Python version satisfies the request: `Python \u0026gt;=3.11` DEBUG Released lock at `/var/folders/dn/vtkw12w17qv7cqw5yj6lmgjh0000gn/T/uv-0492c73d0afa970a.lock` DEBUG Acquired exclusive lock for `~/.cache/uv/environments-v2/whoami-0492c73d0afa970a` DEBUG All requirements satisfied: anyio | certifi | h11\u0026gt;=0.16 | httpcore==1.* | httpx\u0026gt;=0.26 | idna | idna\u0026gt;=2.8 DEBUG Released lock at `~/.cache/uv/environments-v2/whoami-0492c73d0afa970a/.lock` DEBUG Using Python 3.14.2 interpreter at: ~/.cache/uv/environments-v2/whoami-0492c73d0afa970a/bin/python3 DEBUG Running `python whoami.py` DEBUG Spawned child 28443 in process group 28442 Your apparent IP: 5.166.154.224 DEBUG Command exited with code: 0 DEBUG Released lock at `~/.cache/uv/.lock`The line DEBUG Reading inline script metadata from whoami.py shows us that the metadata block is autodetected.\nThe line DEBUG Checking for Python environment at: ~/.cache/uv/environments-v2/whoami-0492c73d0afa970a shows us that uv is looking for a Python environment in the cache directory. This environment is kept stable and is re-used, but is transient in the sense that uv can decide to trash and rebuilt it with a different name any time.\nRunning a URL# We can even decide to run this script as a URL:\n$ uv run -v \u0026#39;https://raw.githubusercontent.com/isotopp/uv-class/refs/heads/main/examples/whoami/whoami.py\u0026#39; DEBUG uv 0.9.21 (0dc9556ad 2025-12-30) DEBUG Acquired shared lock for `/Users/kris/.cache/uv` DEBUG Reading inline script metadata from remote URL DEBUG Acquired exclusive lock for `https://raw.githubusercontent.com/isotopp/uv-class/refs/heads/main/examples/whoami/whoami.py` DEBUG No Python version file found in ancestors of working directory: /Users/kris/Source/uv-class/examples/whoami DEBUG Using Python request Python \u0026gt;=3.11 from `requires-python` metadata DEBUG Checking for Python environment at: `/Users/kris/.cache/uv/environments-v2/ed6cf9cb4d875881` DEBUG The script environment\u0026#39;s Python version satisfies the request: `Python \u0026gt;=3.11` DEBUG Released lock at `/var/folders/dn/vtkw12w17qv7cqw5yj6lmgjh0000gn/T/uv-ed6cf9cb4d875881.lock` DEBUG Acquired exclusive lock for `/Users/kris/.cache/uv/environments-v2/ed6cf9cb4d875881` DEBUG All requirements satisfied: anyio | certifi | h11\u0026gt;=0.16 | httpcore==1.* | httpx\u0026gt;=0.28 | idna | idna\u0026gt;=2.8 DEBUG Released lock at `/Users/kris/.cache/uv/environments-v2/ed6cf9cb4d875881/.lock` DEBUG Using Python 3.14.2 interpreter at: /Users/kris/.cache/uv/environments-v2/ed6cf9cb4d875881/bin/python3 DEBUG Running `python -c` DEBUG Spawned child 28888 in process group 28886 Your apparent IP: 5.166.154.224 DEBUG Command exited with code: 0 DEBUG Released lock at `/Users/kris/.cache/uv/.lock`This is very powerful, and also dangerous (in CI or production). It will download arbitrary Python code from the network, install any dependencies requested and then run that code. There is no way to specifically disable this feature. You can uv run --offline ... or UV_OFFLINE=1 uv run ... to disable any network access, but there is no way to just disable uv-running a URL.\nUsing --with \u0026lt;dependency\u0026gt;# As we saw above, if we try to run whoami-no-dep.py, this fails due to the missing httpx dependency.\nUsing the --with \u0026lt;dependency\u0026gt;, the dependency will be installed into a new, transient environment. The dependency is allowed to violate version constraints.\n$ uv run --with httpx whoami-no-dep.py Your apparent IP: 5.166.154.224Editing inline dependencies# You can create a script with inline dependencies with the --script option:\n$ uv init --script bla.py Initialized script at `bla.py` $ cat bla.py # /// script # requires-python = \u0026#34;\u0026gt;=3.14\u0026#34; # dependencies = [] # /// def main() -\u0026gt; None: print(\u0026#34;Hello from bla.py!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main()Dependencies are added and removed the same way:\n$ uv add --script bla.py httpx Resolved 6 packages in 2ms $ cat bla.py # /// script # requires-python = \u0026#34;\u0026gt;=3.14\u0026#34; # dependencies = [ # \u0026#34;httpx\u0026gt;=0.28.1\u0026#34;, # ] # /// def main() -\u0026gt; None: print(\u0026#34;Hello from bla.py!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main()and\n$ uv remove --script bla.py httpx Updated `bla.py`Running scripts automatically with uv# The magical sheband incantation to run a script through uv is\n#!/usr/bin/env -S uv run --scriptThis will find uv in the path, and execute uv with the parameters listed uv run --script plus the name of the script uv with automatically download a Python interpreter if needed, detect any inline dependency declaration, download the dependencies and run the result.\nThis is rather powerful:\n#!/usr/bin/env -S uv run --script # /// script # requires-python = \u0026#34;\u0026gt;=3.14\u0026#34; # dependencies = [ # \u0026#34;pyqt5\u0026gt;=5.15.11\u0026#34;, # ] # /// import sys from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QGridLayout app = QApplication(sys.argv) widget = QWidget() grid = QGridLayout() text_label = QLabel() text_label.setText(\u0026#34;Hello World!\u0026#34;) grid.addWidget(text_label) widget.setLayout(grid) widget.setGeometry(100, 100, 200, 50) widget.setWindowTitle(\u0026#34;uv\u0026#34;) widget.show() sys.exit(app.exec_())and\n$ ./q.pywill autodownload a full Qt5, and then show a small window:\nMaking a persistent .venv for a script# uv venv uv export --script whoami.py | uv pip sync -The first command creates a .venv directory (optionally using options such as --managed-python and --python 3.12). The second command takes the metadata from the script and generates a lockfile that is written to stdout. The uv pip subcommand sync reads that and processes it, installing the dependencies into the .venv.\n$ uv venv Using CPython 3.14.2 Creating virtual environment at: .venv Activate with: source .venv/bin/activate $ uv export --script whoami.py | uv pip sync - Resolved 7 packages in 6ms Resolved 6 packages in 1ms Installed 6 packages in 5ms + anyio==4.12.1 + certifi==2026.1.4 + h11==0.16.0 + httpcore==1.0.9 + httpx==0.28.1 + idna==3.11 $ ./.venv/bin/python whoami.py Your apparent IP: 5.166.154.224This is not a recommended way for handling an execution environment with uv, but some tooling requires a local virtual environment with a fixed name, and this is a good way to create it. Long term the tooling needs fixing, though.\nExercises# 1. Running Code with uv run# Reproduction: Explain why uv run is preferred over executing python directly from the .venv/bin directory. Reproduction: What does the -- (double-dash) separator do in a uv run command? Application: You have a project with an entrypoint named fetch-data. Write the command to run this entrypoint using Python 3.12, ensuring that any parameters like --limit 10 are passed to the application and not to uv. 2. Editable Installs# Reproduction: What is an \u0026ldquo;editable install,\u0026rdquo; and what is the main benefit for a developer? Reproduction: How does uv handle an editable install for a project using a src layout? Application: You suspect that your project is missing a file that should be included in the final package, but the app works fine in your development environment. How can you use uv run to verify if the app works without the \u0026ldquo;link\u0026rdquo; to your source directory? 3. Tool Execution and uvx# Reproduction: What is the difference between uv run pytest and uvx pytest? Reproduction: Where does uvx store the environments it creates for one-off tool runs? Application: You want to quickly format a single Python file named utils.py using ruff, but ruff is not a dependency in your current project. Write the command to do this without adding ruff to your pyproject.toml. 4. Multiple Environments# Reproduction: How can you test your project against Python 3.12 and 3.13 sequentially using only uv run? Reproduction: What environment variable can you use to tell uv to use a non-default virtual environment directory (e.g., .venv-test)? Application: Create a Makefile entry that runs pytest specifically using a virtual environment directory named .venv-py312 and Python version 3.12. Transfer: Your team is developing a library that must support Python 3.10 through 3.13. Explain how you would set up a local testing strategy that allows you to switch between these versions instantly without the overhead of uv recreating the main .venv every time, and how you would automate this for the team. "},{"id":8,"href":"/books/uv-class/07-python/","title":"Python Management","section":"Using uv to manage python projects","content":"Summary Python Management: uv can automatically download and manage Python interpreters, ensuring consistent environments across machines.\nManaged Python: Using uv-managed Python versions is preferred as it avoids the \u0026ldquo;it works on my machine\u0026rdquo; problem caused by system Python variations.\nPython Selection: uv finds the correct Python version using pyproject.toml, .python-version files, and command-line flags.\nTooling: The uv python subcommand provides tools to install, list, upgrade, and pin Python versions globally or per project.\nWhere does uv get its Python version from?# By default, uv doesn\u0026rsquo;t just look for what is installed on your system. It has the ability to download and manage its own Python interpreters. These interpreters are provided by the python-build-standalone project (maintained by Astral, the creators of uv).\nThese are highly portable, self-contained Python builds that are designed to be unpacked and run from any directory. They are:\nConsistent: Every user on every platform gets the exact same build of a specific Python version. Isolated: They do not interfere with your system Python or other versions. Optimized: They are built with modern compilers and optimizations (like LTO and PGO) for maximum performance. When uv needs a Python version that it doesn\u0026rsquo;t have, it fetches these pre-built binaries, unpacks them into its internal storage, and uses them to create your virtual environments.\nWhy managed Python?# In previous chapters, we emphasized: Never use the system Python.\nThe \u0026ldquo;System Python\u0026rdquo; is the one that comes with your operating system (e.g., /usr/bin/python3 on macOS or Linux). It is there for the OS to run its own scripts. It often has OS-specific patches, non-standard libraries, or pre-installed packages that can lead to subtle bugs in your application.\nBy using Managed Python, you ensure:\nReproducibility: Everyone on the team uses the exact same Python binary. Cleanliness: The interpreter starts with a completely empty site-packages (only the standard library). Flexibility: You can easily switch between Python 3.10, 3.12, 3.14, or even experimental \u0026ldquo;Free-threaded\u0026rdquo; (GIL-less) builds without any complex system setup. Variants:\nStarting with 3.13 (3.14 in as the default), the free-threaded variants of Python become available. Selector: +freethreaded. If you want a variant with a global lock (gil), the selector is +gil. Debug builds of Python are slower and not useful for general use, but a non-optimized version of Python with Debug symbols can be requested as the +debug variant. Enforcing Managed Python# You can tell uv to only use its managed versions and ignore whatever is on your PATH by setting an environment variable or a config option:\nexport UV_PYTHON_PREFERENCE=only-managedOr in your uv.toml:\n[uv] python-preference = \u0026#34;only-managed\u0026#34;This is a great way to ensure that you never accidentally rely on a local system installation.\nValid values for this option are:\nonly-managed: Only use managed Python installations; never use system Python installations. managed: the default, use managed Python installations, if already downloaded. Use System python versions, if matching the requested version. Only download new managed Python if the requested version cannot otherwise be matched. system: Prefer system Python installations over managed Python installations. only-system: Only use system Python installations; never use managed Python installations. Equivalent to \u0026ndash;no-managed-python. Requesting a version# When you create an environment or run a script, you can specify exactly which Python version you want.\nuv venv --python 3.13 uv run --python 3.11 my_script.pyEven if you request a specific version, uv will still respect the requires-python constraint in your pyproject.toml. If you ask for 3.10 but your project requires \u0026gt;=3.12, uv will report an error.\nVersion Specifier Formats# The --python argument is very flexible:\nSimple version: 3.13 (Gets the latest patch of 3.13). Exact version: 3.13.1. Range: '\u0026gt;= 3.10,\u0026lt;3.13' (Finds the best fit within the range). Implementation: cpython@3.12 or pypy@3.10. Variants: 3.13+gil (specifically requests a free-threaded build). System Path: If you must use a specific local Python, you can provide the absolute path: --python /opt/homebrew/bin/python3.12. Disabling Downloads# If you are in an environment where you don\u0026rsquo;t want uv to download anything (e.g., for security or bandwidth reasons), you can disable automatic downloads:\nuv sync --no-python-downloadsThe .python-version file# To avoid typing --python every time, you can \u0026ldquo;pin\u0026rdquo; a version for your project using a .python-version file.\nIf a .python-version file exists in your project root, uv will use that version by default for all commands in that directory.\nGlobal Pins# You can also set a default Python version for your entire user account:\nuv python pin --global 3.12This creates a .python-version file in ~/.config/uv/ (on Unix) or the equivalent configuration directory on your platform. Now, whenever you are not inside a project that specifies its own version, uv will default to 3.12.\nThe uv python toolset# The uv python subcommand provides a set of tools for inspecting and managing your Python installations.\nuv python list# Lists all Python versions available to uv, including those already installed and those available for download.\n$ uv python list cpython-3.14.2-macos-aarch64-none /opt/homebrew/bin/python3.14 -\u0026gt; ../Cellar/python@3.14/3.14.2/bin/python3.14 cpython-3.13.11-macos-aarch64-none ~/.local/share/uv/python/cpython-3.13.11-macos-aarch64-none/bin/python3.13 cpython-3.11.14-macos-aarch64-none \u0026lt;download available\u0026gt; ...The options --managed-python and --no-managed-python select for managed and not managed Python versions, respectively.\nuv python install# Manually download and install specific versions.\n$ uv python install cpython-3.8.20-macos-aarch64-none Installed Python 3.8.20 in 929ms + cpython-3.8.20-macos-aarch64-noneuv python uninstall# Remove a version to save disk space.\n$ uv python uninstall cpython-3.8.20-macos-aarch64-none Searching for Python versions matching: cpython-3.8.20-macos-aarch64-none Uninstalled Python 3.8.20 in 109ms - cpython-3.8.20-macos-aarch64-none (python3.8)uv will only uninstall uv-managed versions of Python. Use the installer for the found Python to uninstall the Python version managed by other systems, for example brew uninstall python#3.8 to uninstall a Python version 3.8 managed by homebrew.\nuv python find# Show the path to the interpreter that uv would use for a given requirement.\n$ pwd ~/Source/uv-class/examples/berlin-weather $ uv python find ~/Source/uv-class/examples/berlin-weather/.venv/bin/python3 $ uv python find 3.8 error: No interpreter found for Python 3.8 in virtual environments, managed installations, or search path $ uv python find 3.11 /opt/homebrew/opt/python@3.11/bin/python3.11uv python dir# Shows where uv stores its managed Python installations.\n$ uv python dir ~/.local/share/uv/pythonuv python upgrade# Updates your managed Python installations to the latest available patch versions.\n$ uv python upgrade warning: `uv python upgrade` is experimental and may change without warning. Pass `--preview-features python-upgrade` to disable this warning All versions already on latest supported patch releaseuv python pin# As seen before, this creates or updates a .python-version file in the current directory (or globally with --global).\nExercises# 1. Python Sources# Reproduction: Where does uv obtain its pre-built Python binaries from by default? Reproduction: List three characteristics of these python-build-standalone interpreters that make them suitable for uv. Application: Run uv python dir on your machine. Explore the directory. What is the internal structure uv uses to store different Python versions? Application: If you are in a highly restricted network environment, what flag or environment variable would you use to prevent uv from trying to download a Python interpreter? 2. Why Managed Python?# Reproduction: Why is it generally discouraged to use the \u0026ldquo;System Python\u0026rdquo; for development projects? Reproduction: What are the three main benefits of using Managed Python as described in this chapter? Application: Configure your environment (either via shell variable or uv.toml) to only allow managed Python versions. Verify this by trying to use a system Python path with uv venv --python /usr/bin/python3. What error does uv return? Application: Explain a scenario where using a uv-managed Python might be problematic, specifically regarding GUI libraries. 3. Requesting a Version# Reproduction: What is the difference between requesting --python 3.13 and --python 3.13.1? Reproduction: How does uv react if you request --python 3.10 for a project that has requires-python = \u0026quot;\u0026gt;=3.12\u0026quot; in its pyproject.toml? Application: Write a command to create a virtual environment using a specific Python implementation (e.g., PyPy) at version 3.10. Application: Use uv run to execute a one-liner that prints the version of a free-threaded (GIL-less) Python build (e.g., 3.13+gil). 4. The .python-version File# Reproduction: What happens when you run a uv command in a directory that contains a .python-version file? Reproduction: How do you set a \u0026ldquo;Global Pin\u0026rdquo; for a Python version, and where is that setting stored? Application: Create a new directory and \u0026ldquo;pin\u0026rdquo; it to Python 3.11 using uv python pin. Verify the content of the created file. Application: You have a global pin of 3.12, but your project directory has a .python-version with 3.13. Which version will uv use when you run uv run python --version inside that project? 5. The uv python Toolset# Reproduction: Which command allows you to see all Python versions uv knows about, including those not yet installed? Reproduction: What is the purpose of uv python update-shell? Application: Use uv python find to locate the interpreter uv would use for the current project. How does this output change if you are inside a project with a .venv versus outside any project? Application: You have several old Python patch versions installed (e.g., 3.12.1, 3.12.2) and want to update them to the latest available patches. Which command should you run, and what is its current status in uv? "},{"id":9,"href":"/books/uv-class/08-build-and-publish/","title":"Building, packaging, and publishing","section":"Using uv to manage python projects","content":"Summary Building Packages: uv build creates source distributions (sdists) and wheels, which are the standard formats for sharing Python code.\nBuild Backends: uv acts as a frontend that coordinates with backends like hatchling, setuptools, or scikit-build-core to perform the actual packaging.\nNative Extensions: For performance-critical code, uv supports building native extensions written in C, Rust, or other languages via specialized backends.\nPublishing: uv publish securely uploads built artifacts to PyPI or private registries, with built-in support for Trusted Publishing and duplicate check protection.\nBest Practices: Always verify builds locally, use version control for tags, and prefer Trusted Publishing in CI/CD environments.\nWhat \u0026ldquo;building\u0026rdquo; means# In the Python world, \u0026ldquo;building\u0026rdquo; a project refers to the process of turning your source code into a standardized, distributable format that others can easily install. This is the bridge between \u0026ldquo;code on your machine\u0026rdquo; and \u0026ldquo;a package on PyPI.\u0026rdquo;\nWhen you build a project, you typically produce two types of artifacts: Source Distributions (sdists) and Binary Distributions (wheels).\nWheels and Source Distributions# Source Distributions (sdist)# An sdist is essentially a snapshot of your source code (usually a .tar.gz file) along with some metadata.\nPros: It is universal; it contains the original code. Cons: It requires the installer to perform the build step. If the project contains C or Rust code, the user must have a compiler and the necessary system libraries installed. Wheels (wheel)# A wheel is a built, \u0026ldquo;ready-to-install\u0026rdquo; format (a .whl file). It is essentially a ZIP archive with a specific structure.\nPros: Installation is nearly instantaneous (just unpacking). No compiler is needed on the user\u0026rsquo;s machine. Cons: For projects with native code, you must build and provide different wheels for every platform (Linux, macOS, Windows) and architecture (x86_64, ARM64) you want to support. Build Backends# Python uses a \u0026ldquo;frontend/backend\u0026rdquo; architecture for packaging, defined in PEP 517.\nFrontend vs. Backend# The Frontend (uv): This is the tool you interact with. uv handles the environment, downloads dependencies, and calls the backend. The Backend: This is the tool that actually knows how to pack your files into a wheel or sdist. Common backends include hatchling, setuptools, maturin, and scikit_build. Why backends exist# Backends exist because different projects have different needs. A pure Python project might use a simple backend like hatchling, while a high-performance project with C++ code might use scikit-build-core or meson-python.\nBy separating the frontend from the backend, uv can support any build system without having to know the specifics of how every language or compiler works.\nNative Extensions Overview# Some Python packages aren\u0026rsquo;t just Python. They contain \u0026ldquo;native extensions\u0026rdquo; written in languages like C, C++, or Rust to achieve performance that Python alone cannot provide.\nWhen uv encounters a project with native extensions, its role remains the same: it provides the isolated build environment and the necessary tools (like cmake or cargo), then lets the specialized backend handle the compilation.\nuv build# uv build is the command that orchestrates the building process.\nuv buildWhen you run this command:\nuv reads your pyproject.toml to see which build-backend you have chosen. It creates an isolated, temporary virtual environment. It installs the required build dependencies (defined in [build-system]). It calls the backend to produce an sdist in the dist/ directory. It then (usually) builds a wheel from that sdist. Useful Flags# --wheel: Build only the wheel. --sdist: Build only the source distribution. --out-dir \u0026lt;DIR\u0026gt;: Change where the artifacts are placed (defaults to dist/). Publishing Packages# Once you have your files in dist/, you can share them with the world (on PyPI) or with your team (on a private registry).\nWhen and what to publish# Publish when: You have a stable, tested version that you want others to use. What to publish: Both the sdist and the wheel. This ensures maximum compatibility for all users. When not to publish# Internal projects: Do not publish private or proprietary code to the public PyPI! Use a private registry or install directly from Git. Broken builds: Always run your tests against the built wheel (using uv run --no-editable) before publishing. uv publish# uv publish is the command used to upload your artifacts to a registry.\nuv publishBy default, it looks for files in the dist/ directory and attempts to upload them to PyPI.\nAuthentication and Best Practices# For security, you should avoid using your raw PyPI password. Instead, use:\nAPI Tokens: Generate a scoped token on PyPI per project and use it with --token. Using a different token per project allows you to scope things appropriately, and also prevents you from mistakenly publishing code to the wrong project. Trusted Publishing: If you are using GitHub Actions, uv supports \u0026ldquo;Trusted Publishing,\u0026rdquo; which uses OIDC tokens to authenticate without needing any secrets stored in your repository. Preventing Duplicates# uv publish is \u0026ldquo;idempotent\u0026rdquo; if you use the --check-url flag (or the --index shorthand). If a file with the exact same name and content already exists on the server, uv will skip it instead of erroring. This is extremely useful for retrying failed CI jobs.\nExample of a robust publish command:\nuv publish --index pypi(This assumes you have configured pypi in your uv.toml with the appropriate publish-url).\nExercises# 1. What \u0026ldquo;building\u0026rdquo; means# Reproduction: What are the two primary types of distribution artifacts produced when building a Python project? Reproduction: Briefly explain the main advantage of a wheel over a source distribution (sdist) for the end-user. Application: You are developing a library that includes a small C extension. Why is it important to provide wheels for multiple platforms, and what happens if a user on an unsupported platform tries to install your package from an sdist? Application: Run uv build on a sample project and look at the dist/ directory. Identify which file is the wheel and which is the sdist based on their file extensions and naming conventions. 2. Build Backends# Reproduction: What is the relationship between a \u0026ldquo;frontend\u0026rdquo; like uv and a \u0026ldquo;backend\u0026rdquo; like hatchling? Reproduction: Which PEP defined the frontend/backend architecture for Python packaging? Application: If you wanted to build a project that uses Rust for performance-critical parts, which build backend might you choose instead of the default uv_build or hatchling? Application: Find the [build-system] section in your project\u0026rsquo;s pyproject.toml file. Explain what the build-backend key specifies and why it is necessary for the build process. 3. Native Extensions Overview# Reproduction: Why would a Python developer choose to write a native extension in C or Rust instead of pure Python? Reproduction: What is the role of uv when it encounters a project that requires a native compilation step? Application: A colleague is trying to install a package with a C extension from an sdist but is getting \u0026ldquo;compiler not found\u0026rdquo; errors. Explain why this is happening and how providing a wheel would solve the issue. Application: Research how uv interacts with tools like cmake or cargo during a build. How does it ensure the build environment is isolated from your system\u0026rsquo;s global tools? 4. uv build# Reproduction: What are the main steps uv takes when you run the uv build command? Reproduction: By default, where does uv build place the resulting artifacts? Application: Write a command that builds only the source distribution of your project and places it in a custom directory named releases/. Application: You want to verify that your package installs correctly without relying on your local source code. How can you use uv run and the --no-editable flag to test the built artifact before publishing? 5. Publishing Packages# Reproduction: Name two situations where you should not publish a package to the public PyPI. Reproduction: Why is it considered a best practice to publish both an sdist and a wheel? Application: You have just finished a new version of your library. Before running uv publish, what testing step should you perform to ensure the built wheel is functional? Application: Explain the difference between publishing to the public PyPI and using a private registry (like a company-internal Artifactory or Nexus) for internal tools. 6. uv publish# Reproduction: What is \u0026ldquo;Trusted Publishing,\u0026rdquo; and how does it improve security in CI/CD environments like GitHub Actions? Reproduction: How does the --check-url flag help when running uv publish in a CI job that might be retried after a partial failure? Application: Write a robust uv publish command that uses a specific index and ensures that duplicate uploads don\u0026rsquo;t cause the command to fail. Application: You are using API tokens for authentication. Why is it better to use a \u0026ldquo;scoped\u0026rdquo; token (restricted to a single project) rather than a global account-wide token? "},{"id":10,"href":"/books/uv-class/09-python-and-c-projects/","title":"Using uv to extend Python with C-code","section":"Using uv to manage python projects","content":"Summary A walkthrough of a uv usage, here to extend Python with two C functions. The functionality of the C extension, \u0026ldquo;hello world\u0026rdquo;, is actually unimportant. We use it to show a build system that can drive a C-compiler, and integrate with Python to build a wheel.\nThis walkthrough builds a small C extension to Python, \u0026ldquo;helloext\u0026rdquo;. It defines two C functions that are being called from Python. The goal is not the helloext funcitonality. Instead we use it to demonstrate a different build system, how to build a Wheel with C extensions, and how to package and install it.\nCreate a project# Our project is called helloext. It will use Python 3.12, and we will be using --managed-python. We are using the usual --package structure, because what we want to write is a package, just written in C.\nmkdir helloext cd helloext uv init --package --managed-python --python 3.12We get the usual tree:\n$ tree . |-- .gitignore |-- .python-version |-- README.md |-- pyproject.toml `-- src `-- helloext `-- __init__.pyOur end state will look a bit differently than before:\n$ tree -a -I .git . |-- .gitignore |-- .python-version |-- CMakeLists.txt |-- README.md |-- pyproject.toml |-- src | `-- helloext | |-- __init__.py | `-- _hello.c |-- tests | `-- test_hello.py `-- uv.lockThat is, we will have our Python code for the module in src/helloext/__init__.py, the C code in _hello.c right next to it.\nWe will be using a different build system that can handle C code: scikit-build-core. This uses cmake, so we need a CmakeLists.txt file.\nTests# We begin with writing tests that define what we want:\n# cat tests/test_hello.py from __future__ import annotations from helloext import hellop, hellos def test_hellos(): assert hellos(\u0026#34;Kris\u0026#34;) == \u0026#34;Hello, Kris\u0026#34; def test_hellop(capsys): hellop(\u0026#34;Kris\u0026#34;) captured = capsys.readouterr() assert captured.out == \u0026#34;Hello, Kris\\n\u0026#34;That is, we will be having an extension helloext that defines two functions, hellop() and hellos().\nOne will be hellop(name: str) -\u0026gt; None. It will take a string name, and print Hello, {name}\\n with a linefeed at the end.\nThe other function will be hellos(name: str) -\u0026gt; str. It will take a string name, and return a string Hello, {name} without a linefeed at the end.\nWe record these facts as tests, using the name \u0026ldquo;Kris\u0026rdquo; for testing. The hellos() assertion is trivial: assert hellos(\u0026quot;Kris\u0026quot;) == \u0026quot;Hello, Kris\u0026quot;. The hellop() uses capsys to capture system output, and then can do the same assertion on captured output.\nWriting the Python part of the extension# Our module, src/helloext is a module, because the directory contains a file named __init__.py. This file imports what is necessary from other submodules, and collects the symbols we want to export with helloext, in the __all__ list.\nThis is how we get an orderly namespace:\nfrom __future__ import annotations from ._hello import hellop, hellos __all__ = [\u0026#34;hellop\u0026#34;, \u0026#34;hellos\u0026#34;]The module ._hello is the C code module in the form of a _hello.so file in the current directory, ..\nThe C code for helloext# We are not going into the details of how to write Python extensions in C in this class. But if you ever saw the structure of an extension in PHP, lua or other languages that integrate C modules, this will look familiar to you:\n// cat src/helloext/_hello.c #define PY_SSIZE_T_CLEAN #include \u0026lt;Python.h\u0026gt; static PyObject *hellop(PyObject *self, PyObject *args) { const char *name = NULL; if (!PyArg_ParseTuple(args, \u0026#34;s\u0026#34;, \u0026amp;name)) { return NULL; } PySys_WriteStdout(\u0026#34;Hello, %s\\n\u0026#34;, name); Py_RETURN_NONE; } static PyObject *hellos(PyObject *self, PyObject *args) { const char *name = NULL; if (!PyArg_ParseTuple(args, \u0026#34;s\u0026#34;, \u0026amp;name)) { return NULL; } return PyUnicode_FromFormat(\u0026#34;Hello, %s\u0026#34;, name); } static PyMethodDef HelloMethods[] = { {\u0026#34;hellop\u0026#34;, hellop, METH_VARARGS, \u0026#34;Print Hello, {name} to stdout.\u0026#34;}, {\u0026#34;hellos\u0026#34;, hellos, METH_VARARGS, \u0026#34;Return Hello, {name} as a string.\u0026#34;}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef hellomodule = { PyModuleDef_HEAD_INIT, \u0026#34;_hello\u0026#34;, \u0026#34;Example CPython extension module.\u0026#34;, -1, HelloMethods }; PyMODINIT_FUNC PyInit__hello(void) { return PyModule_Create(\u0026amp;hellomodule); }Read bottom to top: A module initializer creates a new module, and points to a module definition, hellomodule. The module definition contains, among other metadata, an array of functions, HelloMethods. This NULL terminated array contains structures that define the functions, and the way they get parameters.\nEach function reads Python parameters and converts them into something C can use. We use the Type \u0026ldquo;s\u0026rdquo;, which is a Python \u0026ldquo;utf-8 with surrogates\u0026rdquo; string.\nWe then call appropriate Python API functions such as PySys_WriteStdout() to print, or PyUnicode_FromFormat() to get a formatted dynamic string from memory that Python owns.\nThis code now needs to be compiled: cmake compiles this, using the CMakeLists.txt, and the pyproject.toml tells uv how to call cmake and set it up.\nThe cmake definition# We create a file CMakeLists.txt in the project root:\n# cat CMakeLists.txt cmake_minimum_required(VERSION 3.20) project(helloext LANGUAGES C) find_package(Python REQUIRED COMPONENTS Interpreter Development.Module) Python_add_library(_hello MODULE src/helloext/_hello.c) set_target_properties(_hello PROPERTIES OUTPUT_NAME \u0026#34;_hello\u0026#34; ) if (WIN32) set_target_properties(_hello PROPERTIES SUFFIX \u0026#34;.pyd\u0026#34;) endif() install(TARGETS _hello LIBRARY DESTINATION helloext)This declares minimum version requirements, names project name and language, and finds the required components, basically Python and the needed API includes from the SDK.\nWe create a library, \u0026ldquo;_hello.so\u0026rdquo;, which we tell cmake with Python_add_library.\nWe need to collect the deliverable and install it, that is, we build a Python Wheel and the compiled artifact needs to be added to the Wheel. This is what the install() line does.\nOur pyproject.toml# Our pyproject.toml has a few special options:\n# cat pyproject.toml [project] name = \u0026#34;helloext\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Example CPython C extension with uv + scikit-build-core\u0026#34; readme = \u0026#34;README.md\u0026#34; authors = [ { name = \u0026#34;Kristian Koehntopp\u0026#34;, email = \u0026#34;kris-git@koehntopp.de\u0026#34; } ] requires-python = \u0026#34;\u0026gt;=3.12\u0026#34; dependencies = [] [build-system] requires = [\u0026#34;scikit-build-core\u0026gt;=0.11\u0026#34;, ] build-backend = \u0026#34;scikit_build_core.build\u0026#34; [tool.uv] package = true [tool.scikit-build] cmake.version = \u0026#34;\u0026gt;=3.20\u0026#34; # build.verbose = true wheel.packages = [\u0026#34;src/helloext\u0026#34;] [dependency-groups] dev = [ \u0026#34;ruff\u0026gt;=0.14.10\u0026#34;, ] test = [ \u0026#34;pytest\u0026gt;=9.0.2\u0026#34;, ]We are defining a non-standard [build-system], scikit-build-core.build (with a minimum version requirement).\nWe also define a toml table [tool.scikit-build] to set options:\ncmake.version\u0026gt;=3.20 sets a minimum version requirement for cmake. This must match the requirement in the CMakeLists.txt itself. wheel.packages tells the build system where our package to build are. optionally we can set build.verbose to true to get more build debug output. The other options we should be familiar with by now.\nBuilding# We can now build:\n$ uv build Building source distribution... *** scikit-build-core 0.11.6 (sdist) Building wheel from source distribution... *** scikit-build-core 0.11.6 using CMake 4.2.1 (wheel) *** Configuring CMake... loading initial cache file /var/folders/dn/vtkw12w17qv7cqw5yj6lmgjh0000gn/T/tmpe31qrwbm/build/CMakeInit.txt -- The C compiler identification is AppleClang 17.0.0.17000404 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Found Python: /Users/kris/.cache/uv/builds-v0/.tmp8Kf7xj/bin/python (found version \u0026#34;3.12.12\u0026#34;) found components: Interpreter Development.Module -- Configuring done (0.4s) -- Generating done (0.0s) -- Build files have been written to: /var/folders/dn/vtkw12w17qv7cqw5yj6lmgjh0000gn/T/tmpe31qrwbm/build *** Building project with Ninja... Change Dir: \u0026#39;/var/folders/dn/vtkw12w17qv7cqw5yj6lmgjh0000gn/T/tmpe31qrwbm/build\u0026#39; Run Build Command(s): /Users/kris/.cache/uv/builds-v0/.tmp8Kf7xj/bin/ninja -v [1/2] /usr/bin/cc -D_hello_EXPORTS -isystem /Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/include/python3.12 -O3 -DNDEBUG -arch arm64 -fPIC -MD -MT CMakeFiles/_hello.dir/src/helloext/_hello.c.o -MF CMakeFiles/_hello.dir/src/helloext/_hello.c.o.d -o CMakeFiles/_hello.dir/src/helloext/_hello.c.o -c /Users/kris/.cache/uv/sdists-v9/.tmpCf0QjY/helloext-0.2.0/src/helloext/_hello.c [2/2] : \u0026amp;\u0026amp; /usr/bin/cc -O3 -DNDEBUG -arch arm64 -bundle -Wl,-headerpad_max_install_names -Xlinker -undefined -Xlinker dynamic_lookup -o _hello.so CMakeFiles/_hello.dir/src/helloext/_hello.c.o \u0026amp;\u0026amp; : *** Installing project into wheel... -- Install configuration: \u0026#34;Release\u0026#34; -- Installing: /var/folders/dn/vtkw12w17qv7cqw5yj6lmgjh0000gn/T/tmpe31qrwbm/wheel/platlib/helloext/_hello.so *** Making wheel... *** Created helloext-0.2.0-cp312-cp312-macosx_26_0_arm64.whl Successfully built dist/helloext-0.2.0.tar.gz Successfully built dist/helloext-0.2.0-cp312-cp312-macosx_26_0_arm64.whlThis creates the dist/ directory with two files in it:\nThe dist/helloext-0.2.0.tar.gz with the source distribution for the wheel. The dist/helloext-0.2.0-cp312-cp312-macosx_26_0_arm64.whl, which is the actual wheel. kk:helloext kris$ ls -l dist total 24 -rw-r--r-- 1 kris staff 3338 Jan 6 10:31 helloext-0.2.0-cp312-cp312-macosx_26_0_arm64.whl -rw-r--r-- 1 kris staff 5605 Jan 6 10:31 helloext-0.2.0.tar.gzOur wheel contains the usual platform triple: cp312 for Python 3.12, but this time it\u0026rsquo;s not \u0026ldquo;None-Any\u0026rdquo;, because the C code is platform specific. Instead we get the API cp312 (CPYthon 3.12) in the API field, and \u0026ldquo;macosx_26_0_arm64\u0026rdquo; for the OS field. If our deployment needs to cover more or other platforms, we need to set up a build farm and collect artifacts to spare our users the build.\nA wheel is a ZIP file, so we can unzip -v this:\nkk:helloext kris$ unzip -v dist/helloext-0.2.0-cp312-cp312-macosx_26_0_arm64.whl Archive: dist/helloext-0.2.0-cp312-cp312-macosx_26_0_arm64.whl Length Method Size Cmpr Date Time CRC-32 Name -------- ------ ------- ---- ---------- ----- -------- ---- 287 Defl:N 132 54% 01-06-2026 09:31 0b99c49a helloext/__init__.py 944 Defl:N 434 54% 01-06-2026 09:31 c98a34aa helloext/_hello.c 50232 Defl:N 1409 97% 01-06-2026 09:31 bd8c2210 helloext/_hello.so 239 Defl:N 188 21% 01-06-2026 09:31 4b7d2706 helloext-0.2.0.dist-info/METADATA 114 Defl:N 107 6% 01-06-2026 09:31 6e776e32 helloext-0.2.0.dist-info/WHEEL 434 Defl:N 292 33% 01-06-2026 09:31 fe074413 helloext-0.2.0.dist-info/RECORD -------- ------- --- ------- 52250 2562 95% 6 filesThe important check here is that our wheel contains the __init__.py for the Python part of the module, and the _hello.so for the C/machine level part of the code.\nHad we left out the install() in the CMakeLists.txt file, only the native Python part would be present, but the .so file would be missing.\nInstalling and Reinstalling# uv does not manage Non-Python parts of our environment. We need to install a C-compiler, cmake or ninja (depending on scikit setup), and the required tools and libraries to compile our C code.\nuv also does not manage the freshness of the C code. Because of that we --reinstall or --reinstall-package \u0026lt;packagename\u0026gt; to force a recompile and reinstall of the Wheel:\n$ uv sync --reinstall --all-groups Using CPython 3.12.12 Creating virtual environment at: .venv Resolved 8 packages in 6ms Built helloext @ file:///Users/kris/Source/uv-class/examples/helloext Prepared 7 packages in 1.33s Installed 7 packages in 12ms + helloext==0.2.0 (from file:///Users/kris/Source/uv-class/examples/helloext) + iniconfig==2.3.0 + packaging==25.0 + pluggy==1.6.0 + pygments==2.19.2 + pytest==9.0.2 + ruff==0.14.10We can inspect the .venv to see the extension and its metadata:\nkk:helloext kris$ ls -l .venv/lib/python3.12/site-packages/ total 64 -rw-r--r-- 1 kris staff 58 Jan 6 15:18 _helloext_editable.pth -rw-r--r-- 1 kris staff 9240 Jan 6 15:18 _helloext_editable.py drwxr-xr-x 57 kris staff 1824 Jan 6 15:18 _pytest -rw-r--r-- 1 kris staff 18 Jan 6 15:16 _virtualenv.pth -rw-r--r-- 1 kris staff 4342 Jan 6 15:16 _virtualenv.py drwxr-xr-x 3 kris staff 96 Jan 6 15:18 helloext drwxr-xr-x 10 kris staff 320 Jan 6 15:18 helloext-0.1.0.dist-info drwxr-xr-x 7 kris staff 224 Jan 6 15:18 iniconfig drwxr-xr-x 9 kris staff 288 Jan 6 15:18 iniconfig-2.3.0.dist-info drwxr-xr-x 18 kris staff 576 Jan 6 15:18 packaging drwxr-xr-x 8 kris staff 256 Jan 6 15:18 packaging-25.0.dist-info drwxr-xr-x 11 kris staff 352 Jan 6 15:18 pluggy drwxr-xr-x 9 kris staff 288 Jan 6 15:18 pluggy-1.6.0.dist-info -rw-r--r-- 1 kris staff 329 Dec 14 10:59 py.py drwxr-xr-x 22 kris staff 704 Jan 6 15:18 pygments drwxr-xr-x 9 kris staff 288 Jan 6 15:18 pygments-2.19.2.dist-info drwxr-xr-x 5 kris staff 160 Jan 6 15:18 pytest drwxr-xr-x 10 kris staff 320 Jan 6 15:18 pytest-9.0.2.dist-info drwxr-xr-x 4 kris staff 128 Jan 6 15:18 ruff drwxr-xr-x 8 kris staff 256 Jan 6 15:18 ruff-0.14.10.dist-info kk:helloext kris$ ls -l .venv/lib/python3.12/site-packages/helloext total 104 -rwxr-xr-x 1 kris staff 50232 Jan 6 15:18 _hello.soThe extension is in .venv/lib/python3.12/site-packages/helloext, and we see the _hello.so file in there. The metadata is in the dist-info directory right next to it, with the usual metadata files.\nRunning the tests and using the extension# We then can run out tests as usual, and they will now succeed, using our C code functions instead of native Python:\n$ uv run pytest ============================ test session starts ============================= platform darwin -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0 rootdir: /Users/kris/Source/uv-class/examples/helloext configfile: pyproject.toml collected 2 items tests/test_hello.py .. [100%] ============================= 2 passed in 0.01s ==============================We can also test manually:\n$ uv run python Python 3.12.12 (main, Dec 17 2025, 21:07:08) [Clang 21.1.4 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; from helloext import hellos, hellop \u0026gt;\u0026gt;\u0026gt; hellop(\u0026#34;Kris\u0026#34;) Hello, Kris \u0026gt;\u0026gt;\u0026gt; a = hellos(\u0026#34;Kris\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(a) Hello, KrisWhat can go wrong# Successful build, but No module named 'helloext._hello'# We see a successful build, and even setting the verbose option reveals no errors. Yet, when we try to import the module, we get the message No module named 'helloext._hello'. This is the name of the .so file, _hello.so, so we do get the Python part in the init-file, but not the C component.\nThis is confirmed by listing the contents of the .whl file in dist/ with unzip -v: There is no _hello.so packaged.\nThis happens when we forget the install() clause in the CMakeLists.txt:\ninstall(TARGETS _hello LIBRARY DESTINATION helloext)Editable installs vs. Proper Installs# When we run uv run something, we run it directly from the source directory. The include path that Python uses is expanded, so that modules in src can be found and used. This is called an \u0026ldquo;editable install\u0026rdquo;.\nkk:helloext kris$ uv run python Python 3.12.12 (main, Dec 17 2025, 21:07:08) [Clang 21.1.4 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import pprint \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; pprint.pprint(sys.path) [\u0026#39;\u0026#39;, \u0026#39;/Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python312.zip\u0026#39;, \u0026#39;/Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12\u0026#39;, \u0026#39;/Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/lib-dynload\u0026#39;, \u0026#39;/Users/kris/Source/uv-class/examples/helloext/.venv/lib/python3.12/site-packages\u0026#39;, \u0026#39;/Users/kris/Source/uv-class/examples/helloext/src\u0026#39;]This works with pure python modules, because this will make import helloext search src/helloext, the interpreter will find the module and import it.\nThe C compiler bulld system does the build elsewhere and does not put the module into src/helloext.\nSciKit works around that by putting hooks into site-packages:\nkk:helloext kris$ ls -l .venv/lib/python3.12/site-packages/ total 56 drwxr-xr-x 4 kris staff 128 Jan 6 16:30 __pycache__ -rw-r--r-- 1 kris staff 58 Jan 6 16:42 _helloext_editable.pth -rw-r--r-- 1 kris staff 9240 Jan 6 16:42 _helloext_editable.py -rw-r--r-- 1 kris staff 18 Jan 6 16:30 _virtualenv.pth -rw-r--r-- 1 kris staff 4342 Jan 6 16:30 _virtualenv.py drwxr-xr-x 3 kris staff 96 Jan 6 16:42 helloext drwxr-xr-x 10 kris staff 320 Jan 6 16:42 helloext-0.2.0.dist-info drwxr-xr-x 4 kris staff 128 Jan 6 16:30 ruff drwxr-xr-x 8 kris staff 256 Jan 6 16:30 ruff-0.14.10.dist-infoThe .venv/lib/python\u0026lt;versio\u0026gt;/site-packages/_helloext_editable.py contains Python source that extents the sys.path, but also provides hooks to load .venv/lib/python\u0026lt;versio\u0026gt;/site-packages/helloext/_hello.so.\nIf you check, you will find code that looks similar to this:\nimport sys def ScikitBuildRedirectingFinder(): ... def install( known_source_files: dict[str, str], known_wheel_files: dict[str, str], path: str | None, rebuild: bool = False, verbose: bool = False, build_options: list[str] | None = None, install_options: list[str] | None = None, install_dir: str = \u0026#34;\u0026#34;, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Install a meta path finder that redirects imports to the source files, and optionally rebuilds if path is given. :param known_source_files: A mapping of module names to source files :param known_wheel_files: A mapping of module names to wheel files :param path: The path to the build directory, or None :param verbose: Whether to print the cmake commands (also controlled by the SKBUILD_EDITABLE_VERBOSE environment variable) :param install_dir: The wheel install directory override, if one was specified \u0026#34;\u0026#34;\u0026#34; sys.meta_path.insert( 0, ScikitBuildRedirectingFinder( known_source_files, known_wheel_files, path, rebuild, verbose, build_options or [], install_options or [], DIR, install_dir, ), ) install({\u0026#39;helloext\u0026#39;: \u0026#39;/Users/kris/Source/uv-class/examples/helloext/src/helloext/__init__.py\u0026#39;, \u0026#39;helloext._hello\u0026#39;: \u0026#39;/Users/kris/Source/uv-class/examples/helloext/src/helloext/_hello.c\u0026#39;}, {\u0026#39;helloext._hello\u0026#39;: \u0026#39;helloext/_hello.so\u0026#39;}, None, False, True, [], [], \u0026#39;\u0026#39;)This will try to wire things together even for editable installs.\nRunning uv sync --reinstall-package helloext --no-cache --no-editable will instead install all files into .venv/lib/python\u0026lt;versio\u0026gt;/site-packages/helloext/ as one expects, and also not put src into the search path:\n$ ls -l .venv/lib/python3.12/site-packages/helloext total 120 -rw-r--r-- 1 kris staff 287 Jan 6 16:46 __init__.py -rw-r--r-- 1 kris staff 944 Jan 6 16:46 _hello.c -rwxr-xr-x 1 kris staff 50232 Jan 6 16:46 _hello.so kk:helloext kris$ uv run --no-cache --no-editable python Python 3.12.12 (main, Dec 17 2025, 21:07:08) [Clang 21.1.4 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import pprint \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; pprint.pprint(sys.path) [\u0026#39;\u0026#39;, \u0026#39;/Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python312.zip\u0026#39;, \u0026#39;/Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12\u0026#39;, \u0026#39;/Users/kris/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/lib-dynload\u0026#39;, \u0026#39;/Users/kris/Source/uv-class/examples/helloext/.venv/lib/python3.12/site-packages\u0026#39;]So we can see that SciKit goes to considerable lengths to set up the import path and environment for us, even when we use an editable layout.\nBut \u0026ldquo;editable\u0026rdquo; is a Python thing, and uv scope ends at Python boundaries. We still have to make sure SciKit knows things, and things are being rebuild when we change C sources.\nWe fixed a bug, but the module is still broken# When we fix bugs in the C source, it may be that uv and Scikit work together to produce a new wheel, but in testing the old wheel is still being used.\nWe can work around that by diligently increasing the version number:\nuv version --bump patchto increase the rightmost version number for a versioned compile.\nOr we can tell uv sync, uv run and friends to not use the cache, and force-reinstall\nuv run --all-groups --reinstall-package helloext ...to force a fresh and uncached install.\nWhat we get# We get an extension for Python written in C that can be called like a native Python function.\nIt is delivered as an installable wheel with ABI and OS specific entries in the platform tuple, so we potentially need a build farm to satisfy our supported platforms.\nWe can then import the C-module and use it.\nWe have tooling to drive the C build, but this part of the build is not managed by uv, so it is our responsibility to drive the C part of the build. This includes providing the compiler, the includes and SDKs or other parts of the Non-Python build.\nExercises# 1. Extensions# Reproduction: When initializing the helloext project, which uv init flag is used to establish the src layout, and why is this layout preferred for packages? Reproduction: In the pyproject.toml file for a scikit-build-core project, what are the two keys required in the [build-system] table to define the frontend/backend relationship? Application: You want to see the detailed output of the C compiler and CMake during the build process. Which option should you add or uncomment in the [tool.scikit-build] section of your pyproject.toml? Application: After running uv build, you have a .whl file in the dist/ directory. Provide a command-line example of how you can inspect the contents of this wheel to verify that the compiled _hello.so file is correctly packaged inside the helloext directory. Transfer: Your team is deciding whether to implement a new high-performance module in C using scikit-build-core or in Rust using maturin. Outline how the [build-system] configuration in pyproject.toml would differ between these two choices. Identify which parts of the build stack uv manages (e.g., build isolation, Python dependencies) and which parts remain the developer\u0026rsquo;s responsibility to provide on the host system (e.g., compilers, linker, system SDKs). Discuss how uv handles the \u0026ldquo;freshness\u0026rdquo; of native code compared to pure Python code, and why commands like uv sync --reinstall-package are relevant in this workflow. "},{"id":11,"href":"/books/uv-class/10-collaboration/","title":"Project hygiene and collaboration","section":"Using uv to manage python projects","content":"Summary Project Structure: A consistent layout using src, tests, and docs ensures that your project is legible to both humans and machines.\n.gitignore: Proper ignore rules keep your repository clean of derived artifacts like .venv and __pycache__ while ensuring critical files like uv.lock are tracked.\nDocumentation Strategy: Effective documentation uses the Diátaxis framework to separate intent (tutorials, how-to, reference, explanation) and Bloom\u0026rsquo;s taxonomy to calibrate the cognitive demand of tasks.\nCollaborative Interfaces: README.md acts as the primary user interface for humans, while AGENTS.md provides clear boundaries and instructions for automated tools and AI agents.\nThis chapter focuses on maintainability. It explains structure, documentation, automation boundaries, and collaboration norms. The goal is to make projects legible to other humans and to machines.\nProject structure# A well-structured Python project follows established conventions to make it predictable for developers and automated tools.\nThe src Layout# As discussed in previous chapters, uv init --package defaults to a src layout. This means your actual package code lives inside a src/ directory.\nsrc/: Contains the package directory (e.g., src/berlin_weather/). This prevents accidental imports of your local code when you are in the project root, forcing you to test the code as it will be installed. tests/: Located at the top level, not inside src/. This directory contains your pytest files. Keeping tests separate from source code ensures that tests aren\u0026rsquo;t accidentally packaged and shipped to users. docs/: Contains the project documentation (often using tools like MkDocs or Sphinx). Project Control Files# At the top level of every uv project, you will find several critical files:\npyproject.toml: The authoritative definition of your project, its metadata, and its dependencies. uv.lock: The resolved, deterministic state of your environment. .python-version: The version of Python that uv should use for this project. README.md: The entry point for humans. .gitignore: Instructions for your version control system on what to ignore. .gitignore# The .gitignore file tells Git which files and directories should not be tracked. In a Python project, we want to avoid committing derived artifacts (files that can be regenerated) and environment-specific data.\nWhat to ignore# .venv/: Never commit your virtual environment. It is a derived artifact that uv sync can recreate instantly. It is also platform-specific and contains absolute paths. __pycache__/: These are compiled Python bytecode files (.pyc). They are generated on the fly and are specific to the Python version and machine. dist/ and build/: These contain the artifacts generated by uv build. They should be generated by CI/CD, not committed to Git. .env: This often contains secrets or local configuration. What NOT to ignore# uv.lock: Always commit your lockfile. This is the single most important file for ensuring that everyone on your team is using the exact same versions of all dependencies. .gitignore Syntax Basics# A .gitignore file uses simple globbing patterns:# *: Matches any number of characters (except /). **/: Matches any number of directories. /: At the start, it anchors the pattern to the current directory. !: Negates a pattern (i.e., \u0026ldquo;do not ignore this file\u0026rdquo;). #: Starts a comment. A Good Starting .gitignore# Here is a comprehensive starting point for your .gitignore file. It covers Python-specific files, common IDEs, and typical backup files.\n# --- Python --- __pycache__/ *.py[cod] *$py.class *.so .Python build/ develop-eggs/ dist/ downloads/ eggs/ .eggs/ lib/ lib64/ parts/ sdist/ var/ wheels/ share/python-wheels/ *.egg-info/ .installed.cfg *.egg MANIFEST # --- uv \u0026amp; Environments --- # Never commit the virtual environment or secrets .venv/ .env # --- PyCharm --- .idea/ # --- VS Code --- .vscode/ # Note: You might want to commit .vscode/extensions.json or launch.json # if you want to share workspace settings. # --- Backups and OS stuff --- *.bak *~ *.swp .DS_Store Thumbs.db # --- My Project Specifics --- # Add your own project-specific ignores here Do not ignore uv.lock. As discussed, it must be committed to ensure reproducibility across all environments.\nDocumentation# Documentation is not just about writing; it is about facilitating learning.\nHow we learn: Recall and Spaced Retrieval# Humans are exposed to a lot of information. They are very good at not remembering things to keep their memory clean and efficient. They are also very good at detecting patterns, and at marking things as recurrent, and hence worth remembering. And that is what triggers learning in humans:\nDurable learning happens through recall. This is why the exercises at the end of each chapter in this book are crucial. Instead of just reading, you are forced to retrieve information from memory, which strengthens neural paths.\nRecall: Actively trying to remember something. Spaced Retrieval: Recalling information at increasing intervals. Explanations: Build mental models and reduce \u0026ldquo;blind trial-and-error.\u0026rdquo; Reference: Prevents \u0026ldquo;folklore\u0026rdquo; (knowledge passed by word of mouth that may be wrong or outdated). The Diátaxis Framework# Diátaxis is a framework that organizes documentation by reader intent. It divides documentation into four quadrants:\nTutorials (Learning-oriented): A guided path for a beginner to achieve a small result. Minimal branching, safe environment. How-to Guides (Goal-oriented): Directions for a specific task. \u0026ldquo;How do I add a private registry?\u0026rdquo; Assumes some knowledge. Reference (Information-oriented): Technical description of the machinery. Complete, exact, and neutral. Optimized for lookup. Explanation (Understanding-oriented): Discussions that clarify a topic. Why does uv use a global cache? What are the trade-offs of the src layout? Bloom\u0026rsquo;s Taxonomy# While Diátaxis helps you decide what the page is, Bloom\u0026rsquo;s Taxonomy helps you decide what the reader is asked to do.\nRemember - Understand (Reproduction): Can the reader state facts and explain basic concepts? Apply (Application): Can the reader use the information in a new situation? Analyze - Evaluate - Create (Transfer): Can the reader draw connections, justify a stand, or produce new work? Best Practice:\nUse Diátaxis to decide the structure of your docs. Use Bloom to ensure the difficulty of your exercises matches the intended learning stage. Writing a good README.md# Your README.md is the User Interface of your project. For large projects, it should not contain everything, but rather act as a quick-start guide and a map to the full documentation.\nEvery good README should include:# What it is: A clear, one-sentence description. Who it is for: The target audience. Installation: A simple uv tool install or uv sync command. Quick Start: The minimal amount of code or commands to see it working. Common Tasks: Links to How-to guides for things like running tests or building. Support: Where to go for help (GitHub Issues, Discord, etc.). AGENTS.md: A link to your agent/contributor policy. AGENTS.md# In the age of AI-assisted development, a README.md is no longer enough. An AGENTS.md file serves as an interface for both humans and AI agents (like GitHub Copilot or Junie) who wish to contribute to the project.\nIt defines the \u0026ldquo;Rules of Engagement\u0026rdquo; for the project:\nAutomation Policies# What agents may do unassisted: (e.g., \u0026ldquo;Refactor internal private methods,\u0026rdquo; \u0026ldquo;Update documentation typos\u0026rdquo;). What must never be done automatically: (e.g., \u0026ldquo;Change public API signatures,\u0026rdquo; \u0026ldquo;Modify security-sensitive code\u0026rdquo;). Tooling Boundaries# Allowed Toolchain: \u0026ldquo;We only use uv, never pip.\u0026rdquo; OS Targets: \u0026ldquo;We support Linux and macOS; Windows support is community-best-effort.\u0026rdquo; Formatting: \u0026ldquo;Ruff is the absolute source of truth for style.\u0026rdquo; Consistency Rules# Naming: \u0026ldquo;Public classes must use CamelCase.\u0026rdquo; Error Messages: \u0026ldquo;Use the logging module; never use print for errors.\u0026rdquo; Definition of Done# A checklist of what a PR needs before it can be considered complete:\nTests pass. Documentation updated (following Diátaxis). uv.lock is updated. No new linting warnings. Exercises# 1. Project structure# Reproduction: List the three standard top-level directories in a well-structured Python project and their purposes. Reproduction: What are the three critical \u0026ldquo;control files\u0026rdquo; created by uv at the root of a project, and what does each define? Application: You have a project where your tests are currently inside src/my_package/tests/. Explain why this is problematic according to the chapter and where you should move them. Application: Given a uv project, identify which file you would modify to change the project\u0026rsquo;s metadata (e.g., its version or authors) and which file you should check to see the exact, resolved versions of its dependencies. 2. .gitignore# Reproduction: Why should the .venv/ directory never be committed to a version control system? Reproduction: Explain the purpose of the ! character in a .gitignore file pattern. Application: A team member accidentally commits a large .env file containing database credentials. Explain the steps needed to fix this in the .gitignore and why simply adding the file to .gitignore after it has been committed is not enough. Application: Write a .gitignore pattern that ignores all files with the .bak extension in the entire project, but ensures that a specific file named important_template.bak in the root directory is still tracked. 3. Documentation# Reproduction: Name the four quadrants of the Diátaxis framework and the primary \u0026ldquo;intent\u0026rdquo; of each. Reproduction: What is the psychological difference between \u0026ldquo;Recall\u0026rdquo; and simply reading information, and why is the former preferred for durable learning? Application: You are tasked with writing a page titled \u0026ldquo;How to set up a private PyPI index.\u0026rdquo; According to Diátaxis, which quadrant does this belong to, and should you include extensive theoretical explanations about how indices work on this page? Application: You are designing an exercise that asks a student to \u0026ldquo;Design a deployment strategy for a multi-stage Docker build.\u0026rdquo; According to Bloom\u0026rsquo;s Taxonomy, is this a \u0026ldquo;Reproduction,\u0026rdquo; \u0026ldquo;Application,\u0026rdquo; or \u0026ldquo;Transfer\u0026rdquo; level task? Justify your answer. 4. Writing a good README.md# Reproduction: List at least five essential sections that every good README.md should contain. Reproduction: Why is the README.md described as the \u0026ldquo;User Interface\u0026rdquo; of a project? Application: Critique a README.md that consists only of a single uv sync command. What critical information is missing for a developer who wants to understand what the project actually is? Application: You are starting a new project. Draft a minimal \u0026ldquo;What it is\u0026rdquo; and \u0026ldquo;Quick Start\u0026rdquo; section for its README.md. 5. AGENTS.md# Reproduction: What is an AGENTS.md file, and why is it becoming increasingly important in modern software development? Reproduction: List two examples of \u0026ldquo;Automation Policies\u0026rdquo; that might be defined in an AGENTS.md. Application: Your team uses an AI agent to help with refactoring. Write a sample rule for an AGENTS.md file that specifies what the agent is allowed to do regarding code formatting and which tool it must use as the source of truth. Application: Explain why a \u0026ldquo;Definition of Done\u0026rdquo; in AGENTS.md is particularly useful for automated agents compared to human developers. "},{"id":12,"href":"/books/uv-class/11-uv-migration/","title":"uv in context: migration and judgment","section":"Using uv to manage python projects","content":"Summary uv vs. Traditional Tools: uv replaces pip, venv, and pip-tools with a single, significantly faster, and more reliable tool.\nuv vs. Poetry: While Poetry is a mature all-in-one solution, uv offers superior speed and follows modern standards more closely, though it has slightly different design philosophies regarding environment management.\nTool Boundaries: uv is excellent for project and tool management, but it doesn\u0026rsquo;t replace task runners (like make or just) or complex system-level package managers (like conda or nix) for non-Python dependencies.\nMigration: Moving to uv is straightforward. From pip, it involves initializing a project and adding existing requirements. From Poetry, it means translating pyproject.toml sections and letting uv generate a new lockfile.\nThe final chapter places uv in the wider Python ecosystem. It teaches comparison, trade-offs, and judgment, not tool worship. Students learn when uv is enough and when additional tools are justified.\nuv vs. pip + venv# For years, the standard way to manage Python projects was the combination of pip (to install packages) and venv (to create isolated environments). While this works, it often requires additional tools like pip-tools (for pip-compile and pip-sync) to achieve a reproducible lockfile workflow.\nFeature pip + venv uv Speed Often slow, especially for large dependency trees. Extremely fast (written in Rust, uses aggressive caching). Reproducibility Requires manual management of requirements.txt or pip-compile. Built-in uv.lock ensures identical environments by default. Integration Separate tools for env creation, installation, and locking. Single unified tool for the entire project lifecycle. Python Management Requires external tools like pyenv or manual installation. Automatically downloads and manages Python versions. uv replaces this fragmented workflow with a single, fast, and cohesive experience. You no longer need to remember to \u0026ldquo;activate\u0026rdquo; an environment; uv run handles it for you.\nuv vs. Poetry# Poetry was the first major tool to bring the \u0026ldquo;all-in-one\u0026rdquo; experience to Python, inspired by tools like Rust\u0026rsquo;s cargo or Node\u0026rsquo;s npm. uv follows a similar philosophy but with some key differences.\nFeature Poetry uv Performance Known for slow resolution times in complex projects. Designed for near-instant resolution and installation. Standards Uses its own poetry.lock and (historically) custom pyproject.toml sections. Follows PEP standards (like PEP 621) and uses a standard uv.lock. Environment Manages environments in a central location by default. Defaults to a local .venv in the project directory. Python Management Can switch versions but doesn\u0026rsquo;t download them automatically. Full lifecycle management of Python interpreters. While Poetry is mature and feature-rich, uv is often preferred for its performance and closer adherence to modern Python packaging standards.\nWhen uv is sufficient# uv is an incredibly powerful tool, but it\u0026rsquo;s important to understand its boundaries.\nuv is enough when:# You are developing Python applications or libraries. You need fast, reproducible environments for development and CI/CD. You want to manage Python versions and CLI tools easily. You are building standard wheels and source distributions. You might need other tools when:# Task Running: While uv run can execute anything, it\u0026rsquo;s not a task runner. For complex build pipelines or multi-language projects, use make, just, or task. System Dependencies: uv manages Python packages. If your project requires non-Python libraries (like libxml2, cuda, or ffmpeg) that aren\u0026rsquo;t available as wheels, you still need system package managers (like apt, brew) or conda/pixi. Complex Data Science: In environments where everything (including the GPU drivers and C libraries) must be perfectly versioned together, conda or mamba remains the standard. Migration Principles# Moving an existing project to uv is usually a matter of minutes.\nFrom pip + venv to uv# If you have a project with a requirements.txt or a setup.py:\nInitialize: Run uv init --package (or just uv init for non-packages) in your project root. Add Dependencies: Instead of editing pyproject.toml manually, use uv add to import your existing requirements: uv add -r requirements.txtIf you have development requirements: uv add --dev -r requirements-dev.txt Sync: Run uv sync. This will create the .venv and generate the uv.lock file. Cleanup: You can now remove your old venv directory and requirements.txt files (after verifying everything works). From Poetry to uv# Migrating from Poetry is even easier because Poetry already uses pyproject.toml.\nTranslate Metadata: Poetry uses [tool.poetry] for metadata. You should move this to the standard [project] section. uv can help with this, or you can use a tool like poetry-to-standard. name, version, description, authors go into [project]. dependencies go into [project.dependencies]. dev-dependencies go into [dependency-groups]. Generate Lockfile: uv does not read poetry.lock. Simply run: uv lockuv will resolve the dependencies based on your pyproject.toml and create a new uv.lock. Verify: Run your tests with uv run pytest to ensure the environment is correct. Judgment over Tool Worship# The goal of this book isn\u0026rsquo;t to make you a \u0026ldquo;uv fan.\u0026rdquo; Using uv alone will do that. :-) It\u0026rsquo;s to make you a more productive Python developer.\nThe \u0026ldquo;uv way\u0026rdquo; is about:\nReducing Friction: Spend less time fighting environments and more time writing code. Ensuring Correctness: Use lockfiles and isolated environments to avoid \u0026ldquo;it works on my machine.\u0026rdquo; Embracing Standards: Stick to PEP-compliant configurations so your project remains portable. Whether you use uv, Poetry, or pip, the principles of isolation, reproducibility, and declarative intent remain the most important tools in your belt.\nExercises# 1. uv vs. pip + venv# Reproduction: Name two tools that are often required in a traditional pip + venv workflow to achieve reproducibility, which uv replaces natively. Reproduction: How does uv handle Python version management compared to the traditional pip + venv approach? Application: A developer is tired of manually activating virtual environments. Explain how uv run changes this workflow. Application: Compare the speed and caching mechanisms of pip versus uv when dealing with large dependency trees. 2. uv vs. Poetry# Reproduction: What is the primary difference between Poetry and uv regarding where virtual environments are stored by default? Reproduction: How do Poetry and uv differ in their adherence to modern PEP standards (like PEP 621) for pyproject.toml? Application: A project uses a complex set of legacy poetry.lock files. Can uv use these files directly? Explain why or why not. Application: Compare the dependency resolution performance of Poetry and uv in large-scale projects based on the comparison table in this chapter. 3. When uv is sufficient# Reproduction: List three scenarios where uv is considered sufficient for a Python project\u0026rsquo;s lifecycle. Reproduction: When might you still need a tool like make or just alongside uv? Application: You are working on a project that requires ffmpeg and libxml2. Explain why uv alone is not enough to manage these dependencies and what type of tool you should use instead. Application: A data science project requires specific GPU drivers and C libraries to be perfectly versioned with Python. Why might conda or mamba be a preferred choice over uv in this specific case? 4. Migration Principles# Reproduction: What is the first command you should run when migrating a pip-based project to uv? Reproduction: When migrating from Poetry, which section of the pyproject.toml contains the metadata that must be moved to the standard [project] section? Application: You have a legacy requirements.txt and a requirements-dev.txt. Show the uv commands to import both into a new uv project with appropriate grouping. Application: After translating pyproject.toml metadata from a Poetry project, what is the final step to ensure the environment is correctly set up and verified with uv? "},{"id":13,"href":"/posts/2025-12-22-using-uv/","title":"Using uv to manage python projects","section":"What's new?","content":"This is a site that explains how to manage modern python projects, using uv.\nStart reading\n"}]